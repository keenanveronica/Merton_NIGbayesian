{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d106888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\OneDrive\\Desktop\\ESE\\FCS\\Merton_NIGbayesian\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from data_import import (\n",
    "    load_data, load_ecb_1y_yield,\n",
    "    fill_liabilities, drop_high_leverage_firms,\n",
    "    prepare_merton_inputs\n",
    ")\n",
    "\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83063af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_data] Firms (ret_daily): 46\n",
      "[load_data] Date range (ret_daily): 2012-01-03 .. 2025-12-19\n",
      "[load_data] Coverage min/median/max: 0.999 / 1.000 / 1.000\n",
      "[load_data] liabilities_scale_used: 1e+06\n",
      "[load_data] QA mcap_reported<=0 rows (raw windowed mkt): 62\n",
      "Data has been written to ecb_yc_1y_aaa.xml\n",
      "[drop_high_leverage_firms] agg=median, threshold=8.0\n",
      "[drop_high_leverage_firms] firms before: 46 | after: 36\n",
      "[drop_high_leverage_firms] dropped firms: 10\n"
     ]
    }
   ],
   "source": [
    "# data loading and initial processing\n",
    "ret_daily, bs, coverage = load_data(\n",
    "    Path.cwd() / \"Jan2025_Accenture_Dataset_ErasmusCase.xlsx\",\n",
    "    start_date=\"2012-01-01\",\n",
    "    end_date=\"2025-12-19\",\n",
    "    enforce_coverage=True,\n",
    "    coverage_tol=0.95,\n",
    "    liabilities_scale=\"auto\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "df_rf = load_ecb_1y_yield(\n",
    "    startPeriod=\"2010-01-01\",\n",
    "    endPeriod=\"2025-12-31\",\n",
    "    out_file=\"ecb_yc_1y_aaa.xml\",\n",
    "    verify_ssl=True,  # recommended if it works\n",
    ")\n",
    "\n",
    "df_cal = ret_daily[[\"date\"]].drop_duplicates().sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "debt_daily = fill_liabilities(bs, df_cal)\n",
    "\n",
    "ret_filt, bs_filt, lev_by_firm, dropped = drop_high_leverage_firms(\n",
    "    ret_daily,\n",
    "    bs,\n",
    "    df_calendar=df_cal,\n",
    "    debt_daily=debt_daily,\n",
    "    lev_threshold=8.0,\n",
    "    lev_agg=\"median\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# keep debt panel consistent with filtered firms\n",
    "keep = set(ret_filt[\"gvkey\"].astype(str).unique())\n",
    "debt_daily_filt = debt_daily[debt_daily[\"gvkey\"].astype(str).isin(keep)].copy()\n",
    "\n",
    "# Merton\n",
    "merton_df = prepare_merton_inputs(ret_filt, bs_filt, df_rf, debt_daily=debt_daily_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf891af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sigma_E %: 3.431054858754908\n",
      "count    126655.000000\n",
      "mean          0.254476\n",
      "std           0.083942\n",
      "min           0.117892\n",
      "25%           0.195147\n",
      "50%           0.238274\n",
      "75%           0.292771\n",
      "max           0.743236\n",
      "Name: sigma_E, dtype: float64\n",
      "Missing B %: 0.0\n",
      "Missing r %: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check missingness of sigma_E\n",
    "print(\"Missing sigma_E %:\", merton_df[\"sigma_E\"].isna().mean() * 100)\n",
    "print(merton_df[\"sigma_E\"].describe())\n",
    "\n",
    "# Check missingness of B and r\n",
    "print(\"Missing B %:\", merton_df[\"B\"].isna().mean() * 100)\n",
    "print(\"Missing r %:\", merton_df[\"r\"].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5aad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 131155 Rows after firm-specific start + required inputs: 126655\n",
      "Dropped %: 0.03431054858754908\n",
      "Missing values in calibration dataset:\n",
      "gvkey            0\n",
      "date             0\n",
      "E                0\n",
      "logret_mcap      0\n",
      "isin             0\n",
      "company          0\n",
      "country_iso      0\n",
      "r                0\n",
      "B_drop           0\n",
      "sigma_E_daily    0\n",
      "sigma_E          0\n",
      "calib_start      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# BUILDING THE CALIBRATION DATASET DROPPING ROWS WITH MISSING INPUTS\n",
    "df = merton_df.copy()\n",
    "\n",
    "# first date where B becomes available for each firm\n",
    "first_B_date = (\n",
    "    df.dropna(subset=[\"B\"])\n",
    "      .groupby(\"gvkey\")[\"date\"]\n",
    "      .min()\n",
    "      .rename(\"first_B_date\")\n",
    ")\n",
    "# first date where sigma_E becomes available for each firm\n",
    "first_sigma_date = (\n",
    "    df.dropna(subset=[\"sigma_E\"])\n",
    "      .groupby(\"gvkey\")[\"date\"]\n",
    "      .min()\n",
    "      .rename(\"first_sigma_date\")\n",
    ")\n",
    "\n",
    "starts = pd.concat([first_B_date, first_sigma_date], axis=1)\n",
    "starts[\"calib_start\"] = starts[[\"first_B_date\",\"first_sigma_date\"]].max(axis=1)\n",
    "\n",
    "# attach and filter\n",
    "df2 = df.merge(starts[\"calib_start\"], on=\"gvkey\", how=\"left\")\n",
    "\n",
    "calib = (\n",
    "    df2[df2[\"date\"] >= df2[\"calib_start\"]]\n",
    "      .dropna(subset=[\"E\",\"B\",\"r\",\"sigma_E\"])\n",
    "      .query(\"E > 0 and B > 0\")\n",
    "      .copy()\n",
    "      .rename(columns={\"B\":\"B_drop\"})\n",
    ")\n",
    "\n",
    "print(\"Rows before:\", len(df), \"Rows after firm-specific start + required inputs:\", len(calib))\n",
    "print(\"Dropped %:\", (len(df)-len(calib))/len(df))\n",
    "print(\"Missing values in calibration dataset:\")\n",
    "print(calib.isna().sum())\n",
    "\n",
    "calib_drop = calib.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c02bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firms loaded: 36 | Firms in run: 2\n",
      "Panel date range: 2012-01-03 to 2025-12-19\n",
      "LAST_TRAIN_END: 2024-09-30 | DATA_END: 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "# 5) Rolling settings + one-time preprocessing\n",
    "\n",
    "from merton_model_afonso import (\n",
    "    calibrate_sigmaV_window_weekly_merton,\n",
    "    invert_asset_one_week_merton,\n",
    ")\n",
    "\n",
    "from merton_pd_afonso import (\n",
    "    estimate_mu_from_weekly_implied_assets,\n",
    "    merton_pd_rn_1y,\n",
    "    merton_pd_physical_1y,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Rolling configuration\n",
    "# ----------------------------\n",
    "TRAIN_YEARS = 2\n",
    "STEP_FREQ = \"QE\"               # re-estimate every quarter\n",
    "WEEK_ENDING = \"W-FRI\"         # weekly points ending on Friday (but we use last trading day in that week)\n",
    "T_HORIZON = 1.0               # KMV-style horizon for DD/PD\n",
    "DATA_END = pd.Timestamp(\"2024-12-31\")  # stop at end of dataset\n",
    "MIN_DAILY_ROWS = 10\n",
    "\n",
    "# OOS definition: \"next quarter\"\n",
    "# We'll run windows where the OOS quarter end is <= DATA_END.\n",
    "# So the last train_end will be the quarter end immediately before DATA_END.\n",
    "LAST_TRAIN_END = (DATA_END - pd.offsets.QuarterEnd(1))  # e.g., 2024-09-30\n",
    "\n",
    "# Debug / runtime control: start small, then scale up.\n",
    "MAX_FIRMS = 2   # e.g. 2 for quick test; None for all firms\n",
    "MAX_WINDOWS = None # e.g. 2 for quick test; None for all windows\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# One-time preprocessing for speed\n",
    "# ----------------------------\n",
    "panel = merton_df.copy()\n",
    "panel[\"gvkey\"] = panel[\"gvkey\"].astype(str)\n",
    "panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
    "\n",
    "# Keep only columns we need repeatedly (reduces memory + speeds groupby slicing)\n",
    "needed_cols = [\"gvkey\",\"date\",\"company\",\"E\",\"B\",\"r\",\"sigma_E\"]\n",
    "panel = panel[[c for c in needed_cols if c in panel.columns]].copy()\n",
    "\n",
    "# Coerce numeric once (avoid repeating inside loops)\n",
    "for c in [\"E\",\"B\",\"r\",\"sigma_E\"]:\n",
    "    if c in panel.columns:\n",
    "        panel[c] = pd.to_numeric(panel[c], errors=\"coerce\")\n",
    "\n",
    "# Add constants once\n",
    "panel[\"T\"] = float(T_HORIZON)\n",
    "\n",
    "# Basic cleaning (same spirit as your estimation cell)\n",
    "panel = (\n",
    "    panel.dropna(subset=[\"date\",\"E\",\"B\",\"r\",\"T\"])\n",
    "         .query(\"E > 0 and B > 0\")\n",
    "         .sort_values([\"gvkey\",\"date\"])\n",
    ")\n",
    "\n",
    "# Build per-firm daily panels once (fast slicing later)\n",
    "firm_daily = {}\n",
    "for gvkey, g in panel.groupby(\"gvkey\", sort=False):\n",
    "    g = g.sort_values(\"date\")\n",
    "    # dedupe dates if needed\n",
    "    g = g.groupby(\"date\", as_index=False).last()\n",
    "    firm_daily[gvkey] = g.set_index(\"date\")\n",
    "\n",
    "gvkeys_all = sorted(firm_daily.keys())\n",
    "if MAX_FIRMS is not None:\n",
    "    gvkeys_all = gvkeys_all[:int(MAX_FIRMS)]\n",
    "\n",
    "print(\"Firms loaded:\", len(firm_daily), \"| Firms in run:\", len(gvkeys_all))\n",
    "print(\"Panel date range:\", panel[\"date\"].min().date(), \"to\", panel[\"date\"].max().date())\n",
    "print(\"LAST_TRAIN_END:\", LAST_TRAIN_END.date(), \"| DATA_END:\", DATA_END.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f6bf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>oos_start</th>\n",
       "      <th>oos_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>2014-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2015-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2015-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>2015-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>2016-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2017-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>2017-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>2021-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>2021-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>2022-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>2023-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_start  train_end  oos_start    oos_end\n",
       "0   2012-04-01 2014-03-31 2014-04-01 2014-06-30\n",
       "1   2012-07-01 2014-06-30 2014-07-01 2014-09-30\n",
       "2   2012-10-01 2014-09-30 2014-10-01 2014-12-31\n",
       "3   2013-01-01 2014-12-31 2015-01-01 2015-03-31\n",
       "4   2013-04-01 2015-03-31 2015-04-01 2015-06-30\n",
       "5   2013-07-01 2015-06-30 2015-07-01 2015-09-30\n",
       "6   2013-10-01 2015-09-30 2015-10-01 2015-12-31\n",
       "7   2014-01-01 2015-12-31 2016-01-01 2016-03-31\n",
       "8   2014-04-01 2016-03-31 2016-04-01 2016-06-30\n",
       "9   2014-07-01 2016-06-30 2016-07-01 2016-09-30\n",
       "10  2014-10-01 2016-09-30 2016-10-01 2016-12-31\n",
       "11  2015-01-01 2016-12-31 2017-01-01 2017-03-31\n",
       "12  2015-04-01 2017-03-31 2017-04-01 2017-06-30\n",
       "13  2015-07-01 2017-06-30 2017-07-01 2017-09-30\n",
       "14  2015-10-01 2017-09-30 2017-10-01 2017-12-31\n",
       "15  2016-01-01 2017-12-31 2018-01-01 2018-03-31\n",
       "16  2016-04-01 2018-03-31 2018-04-01 2018-06-30\n",
       "17  2016-07-01 2018-06-30 2018-07-01 2018-09-30\n",
       "18  2016-10-01 2018-09-30 2018-10-01 2018-12-31\n",
       "19  2017-01-01 2018-12-31 2019-01-01 2019-03-31\n",
       "20  2017-04-01 2019-03-31 2019-04-01 2019-06-30\n",
       "21  2017-07-01 2019-06-30 2019-07-01 2019-09-30\n",
       "22  2017-10-01 2019-09-30 2019-10-01 2019-12-31\n",
       "23  2018-01-01 2019-12-31 2020-01-01 2020-03-31\n",
       "24  2018-04-01 2020-03-31 2020-04-01 2020-06-30\n",
       "25  2018-07-01 2020-06-30 2020-07-01 2020-09-30\n",
       "26  2018-10-01 2020-09-30 2020-10-01 2020-12-31\n",
       "27  2019-01-01 2020-12-31 2021-01-01 2021-03-31\n",
       "28  2019-04-01 2021-03-31 2021-04-01 2021-06-30\n",
       "29  2019-07-01 2021-06-30 2021-07-01 2021-09-30\n",
       "30  2019-10-01 2021-09-30 2021-10-01 2021-12-31\n",
       "31  2020-01-01 2021-12-31 2022-01-01 2022-03-31\n",
       "32  2020-04-01 2022-03-31 2022-04-01 2022-06-30\n",
       "33  2020-07-01 2022-06-30 2022-07-01 2022-09-30\n",
       "34  2020-10-01 2022-09-30 2022-10-01 2022-12-31\n",
       "35  2021-01-01 2022-12-31 2023-01-01 2023-03-31\n",
       "36  2021-04-01 2023-03-31 2023-04-01 2023-06-30\n",
       "37  2021-07-01 2023-06-30 2023-07-01 2023-09-30\n",
       "38  2021-10-01 2023-09-30 2023-10-01 2023-12-31\n",
       "39  2022-01-01 2023-12-31 2024-01-01 2024-03-31\n",
       "40  2022-04-01 2024-03-31 2024-04-01 2024-06-30\n",
       "41  2022-07-01 2024-06-30 2024-07-01 2024-09-30\n",
       "42  2022-10-01 2024-09-30 2024-10-01 2024-12-31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Build the rolling quarter schedule\n",
    "\n",
    "global_min_date = panel[\"date\"].min()\n",
    "\n",
    "# earliest possible train_end is >= (min_date + TRAIN_YEARS - 1 day), aligned to quarter-ends\n",
    "earliest_end = (global_min_date + pd.DateOffset(years=TRAIN_YEARS) - pd.Timedelta(days=1))\n",
    "\n",
    "train_ends = pd.date_range(start=earliest_end, end=LAST_TRAIN_END, freq=STEP_FREQ)\n",
    "train_ends = pd.to_datetime(train_ends)\n",
    "\n",
    "if MAX_WINDOWS is not None:\n",
    "    train_ends = train_ends[:int(MAX_WINDOWS)]\n",
    "\n",
    "windows = []\n",
    "for train_end in train_ends:\n",
    "    train_start = train_end - pd.DateOffset(years=TRAIN_YEARS) + pd.Timedelta(days=1)\n",
    "    oos_start = train_end + pd.Timedelta(days=1)\n",
    "    oos_end = train_end + pd.offsets.QuarterEnd(1)  # next quarter end\n",
    "\n",
    "    windows.append({\n",
    "        \"train_start\": pd.Timestamp(train_start),\n",
    "        \"train_end\": pd.Timestamp(train_end),\n",
    "        \"oos_start\": pd.Timestamp(oos_start),\n",
    "        \"oos_end\": pd.Timestamp(oos_end),\n",
    "    })\n",
    "\n",
    "windows_df = pd.DataFrame(windows)\n",
    "windows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e96164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling windows: 100%|██████████| 43/43 [00:04<00:00,  9.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     gvkey train_start  train_end  oos_start    oos_end    ok msg  sigma_hat  \\\n",
       " 0   100022  2012-04-01 2014-03-31 2014-04-01 2014-06-30  True  ok   0.079366   \n",
       " 1   100080  2012-04-01 2014-03-31 2014-04-01 2014-06-30  True  ok   0.140146   \n",
       " 2   100022  2012-07-01 2014-06-30 2014-07-01 2014-09-30  True  ok   0.076176   \n",
       " 3   100080  2012-07-01 2014-06-30 2014-07-01 2014-09-30  True  ok   0.133382   \n",
       " 4   100022  2012-10-01 2014-09-30 2014-10-01 2014-12-31  True  ok   0.077528   \n",
       " ..     ...         ...        ...        ...        ...   ...  ..        ...   \n",
       " 81  100080  2022-04-01 2024-03-31 2024-04-01 2024-06-30  True  ok   0.113557   \n",
       " 82  100022  2022-07-01 2024-06-30 2024-07-01 2024-09-30  True  ok   0.064070   \n",
       " 83  100080  2022-07-01 2024-06-30 2024-07-01 2024-09-30  True  ok   0.107020   \n",
       " 84  100022  2022-10-01 2024-09-30 2024-10-01 2024-12-31  True  ok   0.067516   \n",
       " 85  100080  2022-10-01 2024-09-30 2024-10-01 2024-12-31  True  ok   0.108850   \n",
       " \n",
       "     mu_hat_train         B_end pd_date_is    PD_Q_1y_is    PD_P_1y_is  \\\n",
       " 0       0.075953  1.027250e+11 2014-03-31  3.715969e-08  1.280220e-10   \n",
       " 1       0.199471  3.046400e+10 2014-03-31  1.730767e-20  1.216815e-26   \n",
       " 2       0.100709  1.027250e+11 2014-06-30  7.867343e-09  1.512027e-12   \n",
       " 3       0.189524  3.046400e+10 2014-06-30  1.370358e-23  3.184329e-30   \n",
       " 4       0.076145  1.027250e+11 2014-09-30  1.158223e-07  3.493694e-10   \n",
       " ..           ...           ...        ...           ...           ...   \n",
       " 81     -0.153829  8.318100e+10 2024-03-29  5.166470e-03  1.779648e-01   \n",
       " 82      0.016818  1.579670e+11 2024-06-28  3.858847e-06  1.061129e-05   \n",
       " 83     -0.142161  8.318100e+10 2024-06-28  5.399392e-03  1.758794e-01   \n",
       " 84      0.014945  1.579670e+11 2024-09-30  6.489213e-05  1.131486e-04   \n",
       " 85     -0.081837  8.318100e+10 2024-09-30  2.423922e-03  3.277270e-02   \n",
       " \n",
       "     n_daily_train  n_weekly_train  \n",
       " 0             521             105  \n",
       " 1             521             105  \n",
       " 2             521             105  \n",
       " 3             521             105  \n",
       " 4             522             105  \n",
       " ..            ...             ...  \n",
       " 81            521             105  \n",
       " 82            521             105  \n",
       " 83            521             105  \n",
       " 84            521             105  \n",
       " 85            521             105  \n",
       " \n",
       " [86 rows x 15 columns],\n",
       "            date         V_hat     dlogV         r   gvkey  train_end  \\\n",
       " 0    2012-04-06  1.363874e+11       NaN  0.001453  100022 2014-03-31   \n",
       " 1    2012-04-13  1.368482e+11  0.003373  0.001169  100022 2014-03-31   \n",
       " 2    2012-04-20  1.382167e+11  0.009951  0.001586  100022 2014-03-31   \n",
       " 3    2012-04-27  1.396409e+11  0.010251  0.001112  100022 2014-03-31   \n",
       " 4    2012-05-04  1.377783e+11 -0.013428  0.000824  100022 2014-03-31   \n",
       " ...         ...           ...       ...       ...     ...        ...   \n",
       " 9017 2024-09-06  1.093590e+11  0.009758  0.026990  100080 2024-09-30   \n",
       " 9018 2024-09-13  1.075355e+11 -0.016814  0.026019  100080 2024-09-30   \n",
       " 9019 2024-09-20  1.091761e+11  0.015141  0.026207  100080 2024-09-30   \n",
       " 9020 2024-09-27  1.110398e+11  0.016926  0.024166  100080 2024-09-30   \n",
       " 9021 2024-09-30  1.109666e+11 -0.000660  0.024341  100080 2024-09-30   \n",
       " \n",
       "       sigma_hat  mu_hat_train         B_end  \n",
       " 0      0.079366      0.075953  1.027250e+11  \n",
       " 1      0.079366      0.075953  1.027250e+11  \n",
       " 2      0.079366      0.075953  1.027250e+11  \n",
       " 3      0.079366      0.075953  1.027250e+11  \n",
       " 4      0.079366      0.075953  1.027250e+11  \n",
       " ...         ...           ...           ...  \n",
       " 9017   0.108850     -0.081837  8.318100e+10  \n",
       " 9018   0.108850     -0.081837  8.318100e+10  \n",
       " 9019   0.108850     -0.081837  8.318100e+10  \n",
       " 9020   0.108850     -0.081837  8.318100e+10  \n",
       " 9021   0.108850     -0.081837  8.318100e+10  \n",
       " \n",
       " [9022 rows x 9 columns],\n",
       "        gvkey  train_end       date  sigma_hat  mu_hat_train             E  \\\n",
       " 0     100022 2014-03-31 2014-04-04   0.079366      0.075953  5.649725e+10   \n",
       " 1     100022 2014-03-31 2014-04-11   0.079366      0.075953  5.430599e+10   \n",
       " 2     100022 2014-03-31 2014-04-18   0.079366      0.075953  5.545580e+10   \n",
       " 3     100022 2014-03-31 2014-04-25   0.079366      0.075953  5.391469e+10   \n",
       " 4     100022 2014-03-31 2014-05-02   0.079366      0.075953  5.359563e+10   \n",
       " ...      ...        ...        ...        ...           ...           ...   \n",
       " 1163  100080 2024-09-30 2024-12-06   0.108850     -0.081837  1.966322e+10   \n",
       " 1164  100080 2024-09-30 2024-12-13   0.108850     -0.081837  1.941270e+10   \n",
       " 1165  100080 2024-09-30 2024-12-20   0.108850     -0.081837  1.860318e+10   \n",
       " 1166  100080 2024-09-30 2024-12-27   0.108850     -0.081837  1.894310e+10   \n",
       " 1167  100080 2024-09-30 2024-12-31   0.108850     -0.081837  1.897454e+10   \n",
       " \n",
       "             B_used         r     V_hat_oos  dlogV_oos  mu_hat_oos  \\\n",
       " 0     1.027250e+11  0.001217  1.590973e+11   0.008410    0.020209   \n",
       " 1     1.027250e+11  0.001092  1.569189e+11  -0.013787    0.020209   \n",
       " 2     1.027250e+11  0.000972  1.580810e+11   0.007378    0.020209   \n",
       " 3     1.027250e+11  0.001185  1.565181e+11  -0.009936    0.020209   \n",
       " 4     1.027250e+11  0.000924  1.562258e+11  -0.001869    0.020209   \n",
       " ...            ...       ...           ...        ...         ...   \n",
       " 1163  8.318100e+10  0.022169  1.009324e+11   0.006002   -0.370984   \n",
       " 1164  8.318100e+10  0.022144  1.006784e+11  -0.002520   -0.370984   \n",
       " 1165  8.318100e+10  0.021721  9.988291e+10  -0.007932   -0.370984   \n",
       " 1166  8.318100e+10  0.021885  1.002185e+11   0.003354   -0.370984   \n",
       " 1167  8.318100e+10  0.021786  1.002587e+11   0.000401   -0.370984   \n",
       " \n",
       "        PD_Q_1y_oos   PD_P_1y_oos  \n",
       " 0     2.037122e-08  5.114488e-09  \n",
       " 1     5.412197e-08  1.402631e-08  \n",
       " 2     3.262744e-08  8.204367e-09  \n",
       " 3     6.415138e-08  1.685841e-08  \n",
       " 4     7.424729e-08  1.927174e-08  \n",
       " ...            ...           ...  \n",
       " 1163  2.703382e-02  9.540622e-01  \n",
       " 1164  2.852569e-02  9.562505e-01  \n",
       " 1165  3.391336e-02  9.625939e-01  \n",
       " 1166  3.155068e-02  9.600099e-01  \n",
       " 1167  3.135416e-02  9.596913e-01  \n",
       " \n",
       " [1168 rows x 13 columns])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) Rolling estimation + PDs + OOS inversion\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def last_trading_day_each_week(df_daily_indexed, week_ending=\"W-FRI\"):\n",
    "    \"\"\"\n",
    "    df_daily_indexed: daily df with DatetimeIndex (trading days)\n",
    "    Returns a sorted list of actual trading dates representing the last trading day in each W-FRI week.\n",
    "    \"\"\"\n",
    "    if df_daily_indexed.empty:\n",
    "        return []\n",
    "    tmp = df_daily_indexed.reset_index().rename(columns={\"index\": \"date\"})\n",
    "    tmp[\"week\"] = tmp[\"date\"].dt.to_period(week_ending)\n",
    "    wk = tmp.groupby(\"week\")[\"date\"].max().sort_values()\n",
    "    return wk.tolist()\n",
    "\n",
    "\n",
    "# Rolling outputs\n",
    "roll_summary_rows = []      # firm x window (sigma, mu, status)\n",
    "roll_weekly_is_rows = []    # weekly in-sample (optional but useful)\n",
    "roll_weekly_oos_rows = []   # weekly OOS\n",
    "\n",
    "# Warm-start caches\n",
    "prev_sigma_init = {}  # gvkey -> last sigma_hat\n",
    "prev_lastV = {}       # gvkey -> last in-sample V_hat\n",
    "\n",
    "for w in tqdm(windows, desc=\"Rolling windows\"):\n",
    "    train_start = w[\"train_start\"]\n",
    "    train_end   = w[\"train_end\"]\n",
    "    oos_start   = w[\"oos_start\"]\n",
    "    oos_end     = w[\"oos_end\"]\n",
    "\n",
    "    for gvkey in gvkeys_all:\n",
    "        g_all = firm_daily.get(gvkey)\n",
    "        if g_all is None or g_all.empty:\n",
    "            roll_summary_rows.append({\n",
    "                \"gvkey\": gvkey, \"train_end\": train_end, \"ok\": False, \"msg\": \"missing_firm_panel\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # ----------------------------\n",
    "        # Training slice\n",
    "        # ----------------------------\n",
    "        g_train = g_all.loc[(g_all.index >= train_start) & (g_all.index <= train_end)].copy()\n",
    "        if g_train.empty or len(g_train) < MIN_DAILY_ROWS:\n",
    "            roll_summary_rows.append({\n",
    "                \"gvkey\": gvkey, \"train_start\": train_start, \"train_end\": train_end,\n",
    "                \"ok\": False, \"msg\": \"too_few_daily_rows_train\", \"n_daily\": int(len(g_train))\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Add required columns for calibration function (it expects \"date\" column, not index)\n",
    "        g_train2 = g_train.reset_index().rename(columns={\"index\": \"date\"})\n",
    "        g_train2[\"T\"] = float(T_HORIZON)\n",
    "\n",
    "        # columns are already numeric from preprocessing; keep consistent names\n",
    "        g_train2 = (\n",
    "            g_train2.dropna(subset=[\"date\",\"E\",\"B\",\"r\",\"T\"])\n",
    "                    .query(\"E > 0 and B > 0\")\n",
    "                    .sort_values(\"date\")\n",
    "        )\n",
    "\n",
    "        if len(g_train2) < MIN_DAILY_ROWS:\n",
    "            roll_summary_rows.append({\n",
    "                \"gvkey\": gvkey, \"train_start\": train_start, \"train_end\": train_end,\n",
    "                \"ok\": False, \"msg\": \"too_few_daily_rows_train_after_clean\", \"n_daily\": int(len(g_train2))\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Liability for PD calculation: last available daily point in training window (your approach)\n",
    "        B_end = float(g_train2[\"B\"].iloc[-1])\n",
    "\n",
    "        # Warm start sigma\n",
    "        sigma_init = prev_sigma_init.get(gvkey, None)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Estimate sigma_V + weekly implied assets in-sample\n",
    "        # ----------------------------\n",
    "        sigmaV_hat, weekly_df, ok, msg = calibrate_sigmaV_window_weekly_merton(\n",
    "            g_train2,\n",
    "            week_ending=WEEK_ENDING,\n",
    "            ann_factor=52.0,\n",
    "            sigmaV_init=sigma_init,\n",
    "            E_col=\"E\",\n",
    "            B_col=\"B\",\n",
    "            r_col=\"r\",\n",
    "            T_col=\"T\",\n",
    "            sigmaE_col=\"sigma_E\",\n",
    "        )\n",
    "\n",
    "        if (not ok) or (weekly_df is None) or (len(weekly_df) == 0) or (not np.isfinite(sigmaV_hat)) or (sigmaV_hat <= 0):\n",
    "            roll_summary_rows.append({\n",
    "                \"gvkey\": gvkey, \"train_start\": train_start, \"train_end\": train_end,\n",
    "                \"ok\": False, \"msg\": f\"calibration_failed:{msg}\", \"sigma_hat\": sigmaV_hat\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # store for warm-start next window\n",
    "        prev_sigma_init[gvkey] = float(sigmaV_hat)\n",
    "\n",
    "        # ----------------------------\n",
    "        # In-sample mu and PD at last weekly point\n",
    "        # ----------------------------\n",
    "        w_is = weekly_df.copy()\n",
    "        # expected cols: date, V_hat, dlogV, r (based on your earlier usage)\n",
    "        w_is[\"date\"] = pd.to_datetime(w_is[\"date\"])\n",
    "        w_is = w_is.sort_values(\"date\")\n",
    "\n",
    "        # estimate mu from weekly implied assets\n",
    "        mu_hat = estimate_mu_from_weekly_implied_assets(\n",
    "            pd.DataFrame({\"dlogV\": w_is[\"dlogV\"].to_numpy(dtype=float)}),\n",
    "            float(sigmaV_hat),\n",
    "            ann_factor=52.0\n",
    "        )\n",
    "        mu_hat = float(mu_hat) if np.isfinite(mu_hat) else np.nan\n",
    "\n",
    "        pd_date = pd.Timestamp(w_is[\"date\"].iloc[-1])\n",
    "        V_pd = float(w_is[\"V_hat\"].iloc[-1])\n",
    "        r_pd = float(w_is[\"r\"].iloc[-1])\n",
    "\n",
    "        PD_Q_1y_is = float(merton_pd_rn_1y(V_pd, B_end, r_pd, float(sigmaV_hat)))\n",
    "        PD_P_1y_is = float(merton_pd_physical_1y(V_pd, B_end, mu_hat, float(sigmaV_hat))) if np.isfinite(mu_hat) else np.nan\n",
    "\n",
    "        # store last in-sample V for OOS warm start\n",
    "        prev_lastV[gvkey] = float(V_pd)\n",
    "\n",
    "        roll_summary_rows.append({\n",
    "            \"gvkey\": gvkey,\n",
    "            \"train_start\": train_start, \"train_end\": train_end,\n",
    "            \"oos_start\": oos_start, \"oos_end\": oos_end,\n",
    "            \"ok\": True, \"msg\": \"ok\",\n",
    "            \"sigma_hat\": float(sigmaV_hat),\n",
    "            \"mu_hat_train\": mu_hat,\n",
    "            \"B_end\": B_end,\n",
    "            \"pd_date_is\": pd_date,\n",
    "            \"PD_Q_1y_is\": PD_Q_1y_is,\n",
    "            \"PD_P_1y_is\": PD_P_1y_is,\n",
    "            \"n_daily_train\": int(len(g_train2)),\n",
    "            \"n_weekly_train\": int(len(w_is)),\n",
    "        })\n",
    "\n",
    "        # Optional: store weekly in-sample rows (can be useful for diagnostics)\n",
    "        w_store = w_is[[\"date\",\"V_hat\",\"dlogV\",\"r\"]].copy()\n",
    "        w_store[\"gvkey\"] = gvkey\n",
    "        w_store[\"train_end\"] = train_end\n",
    "        w_store[\"sigma_hat\"] = float(sigmaV_hat)\n",
    "        w_store[\"mu_hat_train\"] = mu_hat\n",
    "        w_store[\"B_end\"] = B_end\n",
    "        roll_weekly_is_rows.append(w_store)\n",
    "\n",
    "        # ----------------------------\n",
    "        # OOS slice + weekly inversion for next quarter\n",
    "        # ----------------------------\n",
    "        g_oos = g_all.loc[(g_all.index >= oos_start) & (g_all.index <= oos_end)].copy()\n",
    "        if g_oos.empty:\n",
    "            continue\n",
    "\n",
    "        # ensure required cols exist (preprocessing should have done this)\n",
    "        g_oos = g_oos.dropna(subset=[\"E\",\"B\",\"r\"]).query(\"E>0 and B>0\").copy()\n",
    "        if g_oos.empty:\n",
    "            continue\n",
    "\n",
    "        weekly_dates = last_trading_day_each_week(g_oos, week_ending=WEEK_ENDING)\n",
    "        if len(weekly_dates) == 0:\n",
    "            continue\n",
    "\n",
    "        # invert week by week\n",
    "        V_prev = prev_lastV.get(gvkey, None)\n",
    "        V_prev = float(V_prev) if (V_prev is not None and np.isfinite(V_prev) and V_prev > 0) else None\n",
    "\n",
    "        oos_rows_this = []\n",
    "        V_prev_for_dlog = V_prev\n",
    "\n",
    "        # fast lookup by date\n",
    "        g_oos_idx = g_oos.groupby(g_oos.index).last()\n",
    "\n",
    "        for d in weekly_dates:\n",
    "            if d not in g_oos_idx.index:\n",
    "                continue\n",
    "            row = g_oos_idx.loc[d]\n",
    "\n",
    "            E_obs = float(row[\"E\"])\n",
    "            B_obs = float(row[\"B\"])\n",
    "            r_obs = float(row[\"r\"])\n",
    "\n",
    "            V_hat, d1, d2 = invert_asset_one_week_merton(\n",
    "                E_obs, B_obs, float(r_obs), float(T_HORIZON), float(sigmaV_hat),\n",
    "                V_prev=V_prev,\n",
    "                tol=1e-6,\n",
    "                maxiter=200\n",
    "            )\n",
    "\n",
    "            # compute weekly log-return on implied assets (if we have a previous)\n",
    "            dlogV = np.nan\n",
    "            if V_prev_for_dlog is not None and np.isfinite(V_prev_for_dlog) and V_prev_for_dlog > 0 and np.isfinite(V_hat) and V_hat > 0:\n",
    "                dlogV = float(np.log(V_hat / V_prev_for_dlog))\n",
    "\n",
    "            V_prev_for_dlog = V_hat\n",
    "            V_prev = V_hat\n",
    "\n",
    "            oos_rows_this.append({\n",
    "                \"gvkey\": gvkey,\n",
    "                \"train_end\": train_end,\n",
    "                \"date\": pd.Timestamp(d),\n",
    "                \"sigma_hat\": float(sigmaV_hat),\n",
    "                \"mu_hat_train\": mu_hat,\n",
    "                \"E\": E_obs,\n",
    "                \"B_used\": B_obs,\n",
    "                \"r\": r_obs,\n",
    "                \"V_hat_oos\": float(V_hat),\n",
    "                \"dlogV_oos\": dlogV,\n",
    "            })\n",
    "\n",
    "        if len(oos_rows_this) == 0:\n",
    "            continue\n",
    "\n",
    "        oos_df_this = pd.DataFrame(oos_rows_this).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "        # estimate ONE mu for the OOS quarter (matches your “get mu for next quarter” wording)\n",
    "        mu_hat_oos = estimate_mu_from_weekly_implied_assets(\n",
    "            pd.DataFrame({\"dlogV\": oos_df_this[\"dlogV_oos\"].to_numpy(dtype=float)}),\n",
    "            float(sigmaV_hat),\n",
    "            ann_factor=52.0\n",
    "        )\n",
    "        mu_hat_oos = float(mu_hat_oos) if np.isfinite(mu_hat_oos) else np.nan\n",
    "        oos_df_this[\"mu_hat_oos\"] = mu_hat_oos\n",
    "\n",
    "        # vectorized PDs\n",
    "        oos_df_this[\"PD_Q_1y_oos\"] = merton_pd_rn_1y(\n",
    "            oos_df_this[\"V_hat_oos\"].to_numpy(float),\n",
    "            oos_df_this[\"B_used\"].to_numpy(float),\n",
    "            oos_df_this[\"r\"].to_numpy(float),\n",
    "            oos_df_this[\"sigma_hat\"].to_numpy(float),\n",
    "        )\n",
    "\n",
    "        oos_df_this[\"PD_P_1y_oos\"] = np.nan\n",
    "        mask = np.isfinite(oos_df_this[\"mu_hat_oos\"].to_numpy(float))\n",
    "        if mask.any():\n",
    "            oos_df_this.loc[mask, \"PD_P_1y_oos\"] = merton_pd_physical_1y(\n",
    "                oos_df_this.loc[mask, \"V_hat_oos\"].to_numpy(float),\n",
    "                oos_df_this.loc[mask, \"B_used\"].to_numpy(float),\n",
    "                oos_df_this.loc[mask, \"mu_hat_oos\"].to_numpy(float),\n",
    "                oos_df_this.loc[mask, \"sigma_hat\"].to_numpy(float),\n",
    "            )\n",
    "\n",
    "        roll_weekly_oos_rows.append(oos_df_this)\n",
    "\n",
    "# Build final rolling outputs\n",
    "roll_summary_df = pd.DataFrame(roll_summary_rows).sort_values([\"train_end\",\"gvkey\"]).reset_index(drop=True)\n",
    "\n",
    "roll_weekly_is_df = (\n",
    "    pd.concat(roll_weekly_is_rows, ignore_index=True)\n",
    "      .sort_values([\"train_end\",\"gvkey\",\"date\"])\n",
    "      .reset_index(drop=True)\n",
    ") if len(roll_weekly_is_rows) else pd.DataFrame()\n",
    "\n",
    "roll_weekly_oos_df = (\n",
    "    pd.concat(roll_weekly_oos_rows, ignore_index=True)\n",
    "      .sort_values([\"train_end\",\"gvkey\",\"date\"])\n",
    "      .reset_index(drop=True)\n",
    ") if len(roll_weekly_oos_rows) else pd.DataFrame()\n",
    "\n",
    "roll_summary_df, roll_weekly_is_df, roll_weekly_oos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d7393d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll_summary_df shape: (4, 15)\n",
      "roll_weekly_is_df shape: (420, 9)\n",
      "roll_weekly_oos_df shape: (56, 13)\n",
      "Unique train_end windows: 2\n",
      "Share ok: 1.0\n",
      "Sigma summary (ok only):\n",
      "count    4.000000\n",
      "mean     0.107267\n",
      "std      0.034196\n",
      "min      0.076176\n",
      "25%      0.078568\n",
      "50%      0.106374\n",
      "75%      0.135073\n",
      "max      0.140146\n",
      "Name: sigma_hat, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>train_end</th>\n",
       "      <th>sigma_hat</th>\n",
       "      <th>mu_hat_train</th>\n",
       "      <th>PD_Q_1y_is</th>\n",
       "      <th>PD_P_1y_is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.075953</td>\n",
       "      <td>3.715969e-08</td>\n",
       "      <td>1.280220e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>0.100709</td>\n",
       "      <td>7.867343e-09</td>\n",
       "      <td>1.512027e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gvkey  train_end  sigma_hat  mu_hat_train    PD_Q_1y_is    PD_P_1y_is\n",
       "0  100022 2014-03-31   0.079366      0.075953  3.715969e-08  1.280220e-10\n",
       "2  100022 2014-06-30   0.076176      0.100709  7.867343e-09  1.512027e-12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>train_end</th>\n",
       "      <th>date</th>\n",
       "      <th>sigma_hat</th>\n",
       "      <th>mu_hat_oos</th>\n",
       "      <th>PD_Q_1y_oos</th>\n",
       "      <th>PD_P_1y_oos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-04</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>2.037122e-08</td>\n",
       "      <td>5.114488e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-11</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>5.412197e-08</td>\n",
       "      <td>1.402631e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>3.262744e-08</td>\n",
       "      <td>8.204367e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-25</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>6.415138e-08</td>\n",
       "      <td>1.685841e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>7.424729e-08</td>\n",
       "      <td>1.927174e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>9.465028e-08</td>\n",
       "      <td>2.458105e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-05-16</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>1.478620e-07</td>\n",
       "      <td>3.865973e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-05-23</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>6.000773e-08</td>\n",
       "      <td>1.489456e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-05-30</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>3.343194e-08</td>\n",
       "      <td>8.016448e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>2.713461e-08</td>\n",
       "      <td>6.392688e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-06-13</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>4.825571e-08</td>\n",
       "      <td>1.132497e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>2.975610e-08</td>\n",
       "      <td>6.905335e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-06-27</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>3.143902e-08</td>\n",
       "      <td>7.281532e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>2.929973e-08</td>\n",
       "      <td>6.782406e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-07-04</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>3.769002e-09</td>\n",
       "      <td>4.966053e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-07-11</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>9.261609e-09</td>\n",
       "      <td>9.947050e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-07-18</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>5.908353e-09</td>\n",
       "      <td>7.056163e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-07-25</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>4.844650e-09</td>\n",
       "      <td>6.009564e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-08-01</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>3.306585e-08</td>\n",
       "      <td>2.642497e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-08-08</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>4.694087e-08</td>\n",
       "      <td>3.452571e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-08-15</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>4.895995e-08</td>\n",
       "      <td>3.566124e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-08-22</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>2.203760e-08</td>\n",
       "      <td>1.932386e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-08-29</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>2.528600e-08</td>\n",
       "      <td>2.116846e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-09-05</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>9.507708e-09</td>\n",
       "      <td>9.702759e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>2.011582e-08</td>\n",
       "      <td>1.744360e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-09-19</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>2.283574e-08</td>\n",
       "      <td>1.917608e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-09-26</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>5.890457e-08</td>\n",
       "      <td>3.955283e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>100022</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>7.006433e-08</td>\n",
       "      <td>4.514027e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gvkey  train_end       date  sigma_hat  mu_hat_oos   PD_Q_1y_oos  \\\n",
       "0   100022 2014-03-31 2014-04-04   0.079366    0.020209  2.037122e-08   \n",
       "1   100022 2014-03-31 2014-04-11   0.079366    0.020209  5.412197e-08   \n",
       "2   100022 2014-03-31 2014-04-18   0.079366    0.020209  3.262744e-08   \n",
       "3   100022 2014-03-31 2014-04-25   0.079366    0.020209  6.415138e-08   \n",
       "4   100022 2014-03-31 2014-05-02   0.079366    0.020209  7.424729e-08   \n",
       "5   100022 2014-03-31 2014-05-09   0.079366    0.020209  9.465028e-08   \n",
       "6   100022 2014-03-31 2014-05-16   0.079366    0.020209  1.478620e-07   \n",
       "7   100022 2014-03-31 2014-05-23   0.079366    0.020209  6.000773e-08   \n",
       "8   100022 2014-03-31 2014-05-30   0.079366    0.020209  3.343194e-08   \n",
       "9   100022 2014-03-31 2014-06-06   0.079366    0.020209  2.713461e-08   \n",
       "10  100022 2014-03-31 2014-06-13   0.079366    0.020209  4.825571e-08   \n",
       "11  100022 2014-03-31 2014-06-20   0.079366    0.020209  2.975610e-08   \n",
       "12  100022 2014-03-31 2014-06-27   0.079366    0.020209  3.143902e-08   \n",
       "13  100022 2014-03-31 2014-06-30   0.079366    0.020209  2.929973e-08   \n",
       "28  100022 2014-06-30 2014-07-04   0.076176   -0.103768  3.769002e-09   \n",
       "29  100022 2014-06-30 2014-07-11   0.076176   -0.103768  9.261609e-09   \n",
       "30  100022 2014-06-30 2014-07-18   0.076176   -0.103768  5.908353e-09   \n",
       "31  100022 2014-06-30 2014-07-25   0.076176   -0.103768  4.844650e-09   \n",
       "32  100022 2014-06-30 2014-08-01   0.076176   -0.103768  3.306585e-08   \n",
       "33  100022 2014-06-30 2014-08-08   0.076176   -0.103768  4.694087e-08   \n",
       "34  100022 2014-06-30 2014-08-15   0.076176   -0.103768  4.895995e-08   \n",
       "35  100022 2014-06-30 2014-08-22   0.076176   -0.103768  2.203760e-08   \n",
       "36  100022 2014-06-30 2014-08-29   0.076176   -0.103768  2.528600e-08   \n",
       "37  100022 2014-06-30 2014-09-05   0.076176   -0.103768  9.507708e-09   \n",
       "38  100022 2014-06-30 2014-09-12   0.076176   -0.103768  2.011582e-08   \n",
       "39  100022 2014-06-30 2014-09-19   0.076176   -0.103768  2.283574e-08   \n",
       "40  100022 2014-06-30 2014-09-26   0.076176   -0.103768  5.890457e-08   \n",
       "41  100022 2014-06-30 2014-09-30   0.076176   -0.103768  7.006433e-08   \n",
       "\n",
       "     PD_P_1y_oos  \n",
       "0   5.114488e-09  \n",
       "1   1.402631e-08  \n",
       "2   8.204367e-09  \n",
       "3   1.685841e-08  \n",
       "4   1.927174e-08  \n",
       "5   2.458105e-08  \n",
       "6   3.865973e-08  \n",
       "7   1.489456e-08  \n",
       "8   8.016448e-09  \n",
       "9   6.392688e-09  \n",
       "10  1.132497e-08  \n",
       "11  6.905335e-09  \n",
       "12  7.281532e-09  \n",
       "13  6.782406e-09  \n",
       "28  4.966053e-06  \n",
       "29  9.947050e-06  \n",
       "30  7.056163e-06  \n",
       "31  6.009564e-06  \n",
       "32  2.642497e-05  \n",
       "33  3.452571e-05  \n",
       "34  3.566124e-05  \n",
       "35  1.932386e-05  \n",
       "36  2.116846e-05  \n",
       "37  9.702759e-06  \n",
       "38  1.744360e-05  \n",
       "39  1.917608e-05  \n",
       "40  3.955283e-05  \n",
       "41  4.514027e-05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8) Sanity checks + quick previews\n",
    "\n",
    "print(\"roll_summary_df shape:\", roll_summary_df.shape)\n",
    "print(\"roll_weekly_is_df shape:\", roll_weekly_is_df.shape)\n",
    "print(\"roll_weekly_oos_df shape:\", roll_weekly_oos_df.shape)\n",
    "\n",
    "# Check that we actually have rolling windows\n",
    "print(\"Unique train_end windows:\", roll_summary_df[\"train_end\"].nunique() if \"train_end\" in roll_summary_df.columns else None)\n",
    "\n",
    "# Basic validity checks\n",
    "if not roll_summary_df.empty:\n",
    "    print(\"Share ok:\", roll_summary_df[\"ok\"].mean() if \"ok\" in roll_summary_df.columns else None)\n",
    "    print(\"Sigma summary (ok only):\")\n",
    "    print(roll_summary_df.loc[roll_summary_df[\"ok\"] == True, \"sigma_hat\"].describe())\n",
    "\n",
    "# Peek one firm across time\n",
    "if not roll_summary_df.empty:\n",
    "    g0 = roll_summary_df.loc[roll_summary_df[\"ok\"] == True, \"gvkey\"].astype(str).unique()\n",
    "    if len(g0):\n",
    "        example = g0[0]\n",
    "        display(\n",
    "            roll_summary_df[(roll_summary_df[\"gvkey\"] == example) & (roll_summary_df[\"ok\"] == True)]\n",
    "            [[\"gvkey\",\"train_end\",\"sigma_hat\",\"mu_hat_train\",\"PD_Q_1y_is\",\"PD_P_1y_is\"]]\n",
    "            .head(15)\n",
    "        )\n",
    "\n",
    "# Peek OOS PDs for same firm\n",
    "if not roll_weekly_oos_df.empty and len(g0):\n",
    "    display(\n",
    "        roll_weekly_oos_df[roll_weekly_oos_df[\"gvkey\"] == example]\n",
    "        [[\"gvkey\",\"train_end\",\"date\",\"sigma_hat\",\"mu_hat_oos\",\"PD_Q_1y_oos\",\"PD_P_1y_oos\"]]\n",
    "        .head(30)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Accenture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
