{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ed29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_import import load_data\n",
    "\n",
    "ret_daily, bs = load_data(\n",
    "    xlsx_path= None,\n",
    "    verbose=True\n",
    ")\n",
    "df_rf= pd.read_csv(\"ecb_riskfree_1y_daily.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "def build_market_rf_panel(\n",
    "    ret_daily: pd.DataFrame,\n",
    "    df_rf: pd.DataFrame,\n",
    "    *,\n",
    "    equity_col: str = \"mcap\",\n",
    "    returns_col: str | None = None,\n",
    "    id_cols=(\"isin\", \"company\", \"country_iso\"),\n",
    "    rf_col: str = \"r_1y\",\n",
    "    require_E_pos: bool = True,\n",
    "    drop_missing_r: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    keep_mkt = [\"gvkey\", \"date\", equity_col]\n",
    "    if returns_col is not None and returns_col in ret_daily.columns:\n",
    "        keep_mkt.append(returns_col)\n",
    "    for c in id_cols:\n",
    "        if c in ret_daily.columns:\n",
    "            keep_mkt.append(c)\n",
    "\n",
    "    mkt = ret_daily[keep_mkt].copy()\n",
    "    mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
    "    mkt[\"gvkey\"] = mkt[\"gvkey\"].astype(str)\n",
    "    mkt[equity_col] = pd.to_numeric(mkt[equity_col], errors=\"coerce\")\n",
    "    if returns_col is not None and returns_col in mkt.columns:\n",
    "        mkt[returns_col] = pd.to_numeric(mkt[returns_col], errors=\"coerce\")\n",
    "    mkt = mkt.sort_values([\"date\", \"gvkey\"]).reset_index(drop=True)\n",
    "\n",
    "    rf = df_rf[[\"date\", rf_col]].copy()\n",
    "    rf[\"date\"] = pd.to_datetime(rf[\"date\"])\n",
    "    rf[rf_col] = pd.to_numeric(rf[rf_col], errors=\"coerce\")\n",
    "    rf = rf.dropna(subset=[\"date\", rf_col]).sort_values([\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    out = pd.merge_asof(mkt, rf, on=\"date\", direction=\"backward\", allow_exact_matches=True)\n",
    "    out = out.rename(columns={equity_col: \"E\", rf_col: \"r\"})\n",
    "\n",
    "    out[\"E\"] = pd.to_numeric(out[\"E\"], errors=\"coerce\")\n",
    "    out[\"r\"] = pd.to_numeric(out[\"r\"], errors=\"coerce\")\n",
    "    if require_E_pos:\n",
    "        out = out[out[\"E\"] > 0].copy()\n",
    "    if drop_missing_r:\n",
    "        out = out.dropna(subset=[\"r\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_discounted_liabilities_daily(\n",
    "    bs: pd.DataFrame,\n",
    "    df_rf: pd.DataFrame,\n",
    "    *,\n",
    "    debt_col: str = \"liabilities_total\",\n",
    "    rf_col: str = \"r_1y\",\n",
    "    publication_col: str = \"final_date\",\n",
    "    gvkey_col: str = \"gvkey\",\n",
    "    ref_rule: str = \"dec31_prev_year\",\n",
    "    bs_date_col: str = \"date\",\n",
    "    daycount: float = 365.0,\n",
    "    compounding: str = \"simple\",\n",
    "    # NEW:\n",
    "    target_max_year: int | None = None,          # e.g. max year in your market panel dates\n",
    "    tail_year_policy: str = \"ffill_face\",              # \"drop\" or \"ffill_face\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds daily discounted liabilities L_pv(t) within each reference year.\n",
    "\n",
    "    If ref_rule=\"dec31_prev_year\", then a publication in year Y is mapped to ref_year=Y-1.\n",
    "\n",
    "    tail_year_policy:\n",
    "      - \"drop\": only build years for which a mapped ref_year exists in bs\n",
    "      - \"ffill_face\": extend each gvkey to target_max_year by carrying forward last known L_face\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1) clean BS ----\n",
    "    cols = [gvkey_col, publication_col, debt_col]\n",
    "    if bs_date_col in bs.columns:\n",
    "        cols.append(bs_date_col)\n",
    "\n",
    "    bs2 = bs[cols].copy()\n",
    "    bs2[gvkey_col] = bs2[gvkey_col].astype(str)\n",
    "    bs2[publication_col] = pd.to_datetime(bs2[publication_col], errors=\"coerce\")\n",
    "    bs2[debt_col] = pd.to_numeric(bs2[debt_col], errors=\"coerce\")\n",
    "    bs2 = bs2.dropna(subset=[publication_col, debt_col])\n",
    "\n",
    "    # reference date\n",
    "    if ref_rule == \"bs_date\" and bs_date_col in bs2.columns:\n",
    "        bs2[\"ref_date\"] = pd.to_datetime(bs2[bs_date_col], errors=\"coerce\")\n",
    "        bs2 = bs2.dropna(subset=[\"ref_date\"])\n",
    "    elif ref_rule == \"dec31_prev_year\":\n",
    "        ref_year = bs2[publication_col].dt.year - 1\n",
    "        bs2[\"ref_date\"] = pd.to_datetime(ref_year.astype(str) + \"-12-31\")\n",
    "    else:\n",
    "        raise ValueError(\"ref_rule must be 'dec31_prev_year' or 'bs_date' (when bs_date_col exists).\")\n",
    "\n",
    "    bs2[\"ref_year\"] = bs2[\"ref_date\"].dt.year\n",
    "    bs2 = bs2.rename(columns={publication_col: \"pub_date\", debt_col: \"L_face\"})\n",
    "\n",
    "    # keep last publication per gvkey-ref_year\n",
    "    bs2 = (\n",
    "        bs2.sort_values([gvkey_col, \"ref_year\", \"pub_date\"])\n",
    "           .groupby([gvkey_col, \"ref_year\"], as_index=False)\n",
    "           .tail(1)\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # ---- 2) optionally extend last year(s) by ffill of L_face ----\n",
    "    if tail_year_policy not in {\"drop\", \"ffill_face\"}:\n",
    "        raise ValueError(\"tail_year_policy must be 'drop' or 'ffill_face'.\")\n",
    "\n",
    "    if tail_year_policy == \"ffill_face\":\n",
    "        if target_max_year is None:\n",
    "            # fallback: extend to last rf year (still an assumption)\n",
    "            target_max_year = int(pd.to_datetime(df_rf[\"date\"]).dt.year.max())\n",
    "\n",
    "        # reindex each gvkey to a full year grid and ffill the face value\n",
    "        out_blocks = []\n",
    "        for gv, g in bs2.groupby(gvkey_col, sort=False):\n",
    "            g = g.sort_values(\"ref_year\").set_index(\"ref_year\")\n",
    "\n",
    "            full_years = pd.Index(range(int(g.index.min()), int(target_max_year) + 1), name=\"ref_year\")\n",
    "            gg = g.reindex(full_years)\n",
    "\n",
    "            gg[gvkey_col] = gv\n",
    "            gg[\"L_face_source_imputed\"] = gg[\"L_face\"].isna()\n",
    "\n",
    "            # forward-fill L_face and pub_date (pub_date indicates staleness)\n",
    "            gg[\"L_face\"] = gg[\"L_face\"].ffill()\n",
    "            gg[\"pub_date\"] = gg[\"pub_date\"].ffill()\n",
    "\n",
    "            # rebuild ref_date consistently\n",
    "            gg[\"ref_date\"] = pd.to_datetime(gg.index.astype(str) + \"-12-31\")\n",
    "\n",
    "            # drop years where we *still* don't have any L_face (e.g. firm starts after)\n",
    "            gg = gg.dropna(subset=[\"L_face\"])\n",
    "\n",
    "            out_blocks.append(gg.reset_index())\n",
    "\n",
    "        bs2 = pd.concat(out_blocks, ignore_index=True)\n",
    "\n",
    "    # if \"drop\", do nothing: we keep only years that exist\n",
    "\n",
    "    # ---- 3) risk-free calendar ----\n",
    "    rf = df_rf[[\"date\", rf_col]].copy()\n",
    "    rf[\"date\"] = pd.to_datetime(rf[\"date\"])\n",
    "    rf[rf_col] = pd.to_numeric(rf[rf_col], errors=\"coerce\")\n",
    "    rf = rf.dropna(subset=[\"date\", rf_col]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    min_year = int(bs2[\"ref_year\"].min())\n",
    "    max_year = int(bs2[\"ref_year\"].max())\n",
    "\n",
    "    cal = pd.DataFrame({\"date\": pd.date_range(f\"{min_year}-01-01\", f\"{max_year}-12-31\", freq=\"D\")})\n",
    "    cal = pd.merge_asof(cal.sort_values(\"date\"), rf, on=\"date\", direction=\"backward\", allow_exact_matches=True)\n",
    "    cal[\"year\"] = cal[\"date\"].dt.year\n",
    "\n",
    "    # ---- 4) DF(t -> Dec31) ----\n",
    "    df_list = []\n",
    "    dt = 1.0 / daycount\n",
    "\n",
    "    for y, g in cal.groupby(\"year\", sort=True):\n",
    "        r = g[rf_col].to_numpy(dtype=float)\n",
    "        n = len(r)\n",
    "        DF = np.empty(n, dtype=float)\n",
    "        DF[-1] = 1.0\n",
    "\n",
    "        if compounding == \"simple\":\n",
    "            for i in range(n - 2, -1, -1):\n",
    "                DF[i] = DF[i + 1] / (1.0 + r[i] * dt)\n",
    "        elif compounding == \"continuous\":\n",
    "            acc = 0.0\n",
    "            for i in range(n - 2, -1, -1):\n",
    "                acc += r[i] * dt\n",
    "                DF[i] = np.exp(-acc)\n",
    "        else:\n",
    "            raise ValueError(\"compounding must be 'simple' or 'continuous'.\")\n",
    "\n",
    "        tmp = g[[\"date\"]].copy()\n",
    "        tmp[\"ref_year\"] = int(y)\n",
    "        tmp[\"DF_to_dec31\"] = DF\n",
    "        df_list.append(tmp)\n",
    "\n",
    "    DF_table = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # ---- 5) firm-year × daily DF ----\n",
    "    liab = bs2[[gvkey_col, \"ref_year\", \"ref_date\", \"pub_date\", \"L_face\"]].copy()\n",
    "    out = liab.merge(DF_table, on=\"ref_year\", how=\"left\")\n",
    "\n",
    "    out[\"L_pv\"] = out[\"L_face\"] * out[\"DF_to_dec31\"]\n",
    "\n",
    "    out = out.rename(columns={gvkey_col: \"gvkey\"})\n",
    "    out = out[[\"gvkey\", \"date\", \"L_face\", \"L_pv\", \"ref_date\", \"pub_date\", \"ref_year\"]].sort_values([\"gvkey\", \"date\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def attach_debt_daily(\n",
    "    panel: pd.DataFrame,\n",
    "    debt_daily: pd.DataFrame,\n",
    "    *,\n",
    "    debt_out_col: str = \"B\",\n",
    "    pv_col: str = \"L_pv\",   # whatever name your debt builder uses\n",
    ") -> pd.DataFrame:\n",
    "    dd = debt_daily[[\"gvkey\", \"date\", pv_col]].copy()\n",
    "    dd[\"gvkey\"] = dd[\"gvkey\"].astype(str)\n",
    "    dd[\"date\"] = pd.to_datetime(dd[\"date\"])\n",
    "    out = panel.merge(dd, on=[\"gvkey\", \"date\"], how=\"left\")\n",
    "    out = out.rename(columns={pv_col: debt_out_col})\n",
    "    return out\n",
    "\n",
    "\n",
    "# Merton specific functions\n",
    "def equity_volatility(\n",
    "    merton_inputs: pd.DataFrame,\n",
    "    ret_col: str = \"logret_mcap\",\n",
    "    window: int = 252,\n",
    "    min_obs: int = 126,\n",
    "    trading_days: int = 252\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds rolling equity volatility sigma_E(t) (annualized) computed from daily log returns.\n",
    "\n",
    "    sigma_E_daily(t) = rolling std of log returns over `window` within each firm\n",
    "    sigma_E_ann(t)   = sigma_E_daily(t) * sqrt(trading_days)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window : int\n",
    "        Rolling window length in trading days (252 ~ 1 year).\n",
    "    min_obs : int\n",
    "        Minimum observations required to compute rolling std (stability rule).\n",
    "    \"\"\"\n",
    "\n",
    "    df = merton_inputs.copy()\n",
    "    df = df.sort_values([\"gvkey\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # daily rolling std by firm\n",
    "    df[\"sigma_E_daily\"] = (\n",
    "        df.groupby(\"gvkey\")[ret_col]\n",
    "          .transform(lambda s: s.rolling(window=window, min_periods=min_obs).std())\n",
    "    )\n",
    "\n",
    "    # annualize\n",
    "    df[\"sigma_E\"] = df[\"sigma_E_daily\"] * np.sqrt(trading_days)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# NIG specific functions\n",
    "def make_em_inputs(\n",
    "    nig_panel: pd.DataFrame,\n",
    "    gvkey: str,\n",
    "    *,\n",
    "    end_date: str | pd.Timestamp | None = None,\n",
    "    window: int = 505,\n",
    "    use_filled_L: bool = True,\n",
    "    L_pick: str = \"last\",   # \"last\" is the most natural for 'as-of end of window'\n",
    ") -> tuple[np.ndarray, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract (equity_array, L_scalar, rf_array) in the exact shape expected by\n",
    "    nig_em_paper.em_init_nig_params(...) :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "    window=505 matches the empirical setup in Jovan & Ahčan :contentReference[oaicite:3]{index=3}\n",
    "    \"\"\"\n",
    "    df = nig_panel.copy()\n",
    "    df[\"gvkey\"] = df[\"gvkey\"].astype(str)\n",
    "    df = df[df[\"gvkey\"] == str(gvkey)].sort_values(\"date\")\n",
    "\n",
    "    if end_date is not None:\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        df = df[df[\"date\"] <= end_date]\n",
    "\n",
    "    if len(df) < 3:\n",
    "        raise ValueError(\"Not enough observations after filtering (need >= 3).\")\n",
    "\n",
    "    df = df.tail(int(window)).copy()\n",
    "\n",
    "    # arrays\n",
    "    E = df[\"E\"].to_numpy(dtype=float)\n",
    "    r = df[\"r\"].to_numpy(dtype=float)\n",
    "\n",
    "    # liabilities scalar (constant L over the window)\n",
    "    L_col = \"L_filled\" if (use_filled_L and \"L_filled\" in df.columns) else \"L\"\n",
    "    if L_pick == \"last\":\n",
    "        L = float(df[L_col].dropna().iloc[-1])\n",
    "    elif L_pick == \"median\":\n",
    "        L = float(df[L_col].dropna().median())\n",
    "    else:\n",
    "        raise ValueError(\"L_pick must be 'last' or 'median'.\")\n",
    "\n",
    "    # sanity checks (the EM code will also enforce these)\n",
    "    if np.any(~np.isfinite(E)) or np.any(E <= 0.0):\n",
    "        raise ValueError(\"Equity array contains non-finite or non-positive values.\")\n",
    "    if not np.isfinite(L) or L <= 0.0:\n",
    "        raise ValueError(\"Liabilities L must be finite and > 0.\")\n",
    "    if np.any(~np.isfinite(r)):\n",
    "        raise ValueError(\"Risk-free array contains non-finite values.\")\n",
    "    if E.shape != r.shape:\n",
    "        raise ValueError(\"E and r must have identical shapes.\")\n",
    "\n",
    "    return E, L, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea0fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Dict, Any\n",
    "\n",
    "def prepare_merton_inputs(\n",
    "    ret_daily: pd.DataFrame,\n",
    "    bs: pd.DataFrame,\n",
    "    df_rf: pd.DataFrame,\n",
    "    *,\n",
    "    equity_col: str = \"mcap\",\n",
    "    returns_col: str = \"logret_mcap\",\n",
    "    id_cols=(\"isin\", \"company\", \"country_iso\"),\n",
    "    add_sigma_E: bool = True,\n",
    "    sigma_window: int = 252,\n",
    "    sigma_min_obs: int = 126,\n",
    "    trading_days: int = 252,\n",
    "    drop_missing_r: bool = True,\n",
    "    ref_rule: str = \"dec31_prev_year\",\n",
    "    daycount: float = 365.0,\n",
    "    compounding: str = \"simple\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct a Merton input panel from separate daily returns and\n",
    "    balance‑sheet data.  This wrapper calls the market/rf builder,\n",
    "    constructs daily discounted liabilities, attaches them as 'B' and\n",
    "    optionally computes rolling equity volatility.  It avoids as‑of\n",
    "    merging of balance‑sheet values and does not fill missing B.  The\n",
    "    returned DataFrame contains columns:\n",
    "      gvkey, date, E, (returns_col), optional ID columns, B, r,\n",
    "      and optionally sigma_E_daily and sigma_E if add_sigma_E=True.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) build market + risk free panel (includes E and r and returns_col)\n",
    "    market = build_market_rf_panel(\n",
    "        ret_daily=ret_daily,\n",
    "        df_rf=df_rf,\n",
    "        equity_col=equity_col,\n",
    "        returns_col=returns_col,\n",
    "        id_cols=id_cols,\n",
    "        rf_col=\"r_1y\",\n",
    "        require_E_pos=True,\n",
    "        drop_missing_r=drop_missing_r,\n",
    "    )\n",
    "\n",
    "    # 2) build discounted liabilities per firm-year\n",
    "    debt_daily = build_discounted_liabilities_daily(\n",
    "        bs=bs,\n",
    "        df_rf=df_rf,\n",
    "        debt_col=\"liabilities_total\",\n",
    "        rf_col=\"r_1y\",\n",
    "        publication_col=\"final_date\",\n",
    "        gvkey_col=\"gvkey\",\n",
    "        ref_rule=ref_rule,\n",
    "        bs_date_col=\"date\",\n",
    "        daycount=daycount,\n",
    "        compounding=compounding,\n",
    "    )\n",
    "\n",
    "    # 3) attach discounted liabilities to the market panel\n",
    "    df = attach_debt_daily(\n",
    "        panel=market,\n",
    "        debt_daily=debt_daily,\n",
    "        debt_out_col=\"B\",\n",
    "        pv_col=\"L_pv\",\n",
    "    )\n",
    "\n",
    "    # 4) optionally compute rolling equity volatility\n",
    "    if add_sigma_E and returns_col in df.columns:\n",
    "        df = equity_volatility(\n",
    "            df,\n",
    "            ret_col=returns_col,\n",
    "            window=sigma_window,\n",
    "            min_obs=sigma_min_obs,\n",
    "            trading_days=trading_days,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_nig_inputs(\n",
    "    ret_daily: pd.DataFrame,\n",
    "    bs: pd.DataFrame,\n",
    "    df_rf: pd.DataFrame,\n",
    "    *,\n",
    "    equity_col: str = \"mcap\",\n",
    "    id_cols=(\"isin\", \"company\", \"country_iso\"),\n",
    "    build_em: bool = False,\n",
    "    em_window: int = 505,\n",
    "    em_use_filled_L: bool = True,\n",
    "    em_L_pick: str = \"last\",\n",
    "    drop_missing_r: bool = True,\n",
    "    ref_rule: str = \"dec31_prev_year\",\n",
    "    daycount: float = 365.0,\n",
    "    compounding: str = \"simple\",\n",
    ") -> Tuple[pd.DataFrame, Optional[Dict[Tuple[str, pd.Timestamp], Tuple[Any, Any, Any]]]]:\n",
    "    \"\"\"\n",
    "    Construct a NIG input panel from separate daily returns and balance\n",
    "    sheet data.  This wrapper builds the market/rf panel (without\n",
    "    returns), attaches discounted liabilities as 'L' and optionally\n",
    "    extracts EM windows.  Note that no volatility is computed here.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) build market + risk free panel (without returns)\n",
    "    market = build_market_rf_panel(\n",
    "        ret_daily=ret_daily,\n",
    "        df_rf=df_rf,\n",
    "        equity_col=equity_col,\n",
    "        returns_col=None,\n",
    "        id_cols=id_cols,\n",
    "        rf_col=\"r_1y\",\n",
    "        require_E_pos=True,\n",
    "        drop_missing_r=drop_missing_r,\n",
    "    )\n",
    "\n",
    "    # 2) build discounted liabilities per firm-year\n",
    "    debt_daily = build_discounted_liabilities_daily(\n",
    "        bs=bs,\n",
    "        df_rf=df_rf,\n",
    "        debt_col=\"liabilities_total\",\n",
    "        rf_col=\"r_1y\",\n",
    "        publication_col=\"final_date\",\n",
    "        gvkey_col=\"gvkey\",\n",
    "        ref_rule=ref_rule,\n",
    "        bs_date_col=\"date\",\n",
    "        daycount=daycount,\n",
    "        compounding=compounding,\n",
    "    )\n",
    "\n",
    "    # 3) attach discounted liabilities to the market panel as L\n",
    "    df = attach_debt_daily(\n",
    "        panel=market,\n",
    "        debt_daily=debt_daily,\n",
    "        debt_out_col=\"L\",\n",
    "        pv_col=\"L_pv\",\n",
    "    )\n",
    "\n",
    "    # convert to numeric and drop non‑positive L (EM requires positive L)\n",
    "    df[\"L\"] = pd.to_numeric(df[\"L\"], errors=\"coerce\")\n",
    "    df = df[df[\"L\"] > 0].copy()\n",
    "\n",
    "    em_inputs: Optional[Dict[Tuple[str, pd.Timestamp], Tuple[Any, Any, Any]]] = None\n",
    "    if build_em:\n",
    "        em_inputs = {}\n",
    "        # iterate by firm and build windows\n",
    "        for gv, g in df.groupby(\"gvkey\", sort=False):\n",
    "            g = g.sort_values(\"date\")\n",
    "            if len(g) < em_window:\n",
    "                continue\n",
    "            for end_date in g[\"date\"].iloc[em_window - 1:]:\n",
    "                try:\n",
    "                    E_arr, L_scalar, r_arr = make_em_inputs(\n",
    "                        nig_panel=df,\n",
    "                        gvkey=str(gv),\n",
    "                        end_date=end_date,\n",
    "                        window=em_window,\n",
    "                        use_filled_L=em_use_filled_L,\n",
    "                        L_pick=em_L_pick,\n",
    "                    )\n",
    "                    em_inputs[(str(gv), pd.to_datetime(end_date))] = (E_arr, L_scalar, r_arr)\n",
    "                except Exception:\n",
    "                    # skip windows with insufficient or invalid data\n",
    "                    continue\n",
    "    return df, em_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a85ace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merton panel shape: (198390, 11)\n",
      "NIG panel shape: (185744, 8)\n",
      "----------------------------------------\n",
      "    gvkey       date             E  logret_mcap          isin  \\\n",
      "0  100022 2010-01-05  1.945046e+10     0.008080  DE0005190003   \n",
      "1  100022 2010-01-06  1.975146e+10     0.015357  DE0005190003   \n",
      "2  100022 2010-01-07  1.992604e+10     0.008800  DE0005190003   \n",
      "3  100022 2010-01-08  1.965815e+10    -0.013535  DE0005190003   \n",
      "4  100022 2010-01-11  1.936619e+10    -0.014964  DE0005190003   \n",
      "\n",
      "                        company country_iso         r             B  \\\n",
      "0  BAYERISCHE MOTOREN WERKE AKT         DEU  0.007934  8.526755e+10   \n",
      "1  BAYERISCHE MOTOREN WERKE AKT         DEU  0.007782  8.526941e+10   \n",
      "2  BAYERISCHE MOTOREN WERKE AKT         DEU  0.007491  8.527122e+10   \n",
      "3  BAYERISCHE MOTOREN WERKE AKT         DEU  0.007417  8.527297e+10   \n",
      "4  BAYERISCHE MOTOREN WERKE AKT         DEU  0.007056  8.527817e+10   \n",
      "\n",
      "   sigma_E_daily  sigma_E  \n",
      "0            NaN      NaN  \n",
      "1            NaN      NaN  \n",
      "2            NaN      NaN  \n",
      "3            NaN      NaN  \n",
      "4            NaN      NaN  \n",
      "----------------------------------------\n",
      "    gvkey       date             E          isin  \\\n",
      "0  100022 2010-01-05  1.945046e+10  DE0005190003   \n",
      "1  100080 2010-01-05  4.578810e+10  DE000BAY0017   \n",
      "2  100312 2010-01-05  1.765719e+09  DE0007030009   \n",
      "3  100581 2010-01-05  4.701386e+10  FR0000120321   \n",
      "4  100737 2010-01-05  6.656321e+09  DE0007664039   \n",
      "\n",
      "                        company country_iso         r             L  \n",
      "0  BAYERISCHE MOTOREN WERKE AKT         DEU  0.007934  8.526755e+10  \n",
      "1                      BAYER AG         DEU  0.007934  3.235349e+10  \n",
      "2                RHEINMETALL AG         DEU  0.007934  3.086919e+09  \n",
      "3                     LOREAL SA         FRA  0.007934  9.125250e+09  \n",
      "4                 VOLKSWAGEN AG         DEU  0.007934  1.498035e+11  \n"
     ]
    }
   ],
   "source": [
    "df_merton = prepare_merton_inputs(ret_daily, bs, df_rf)\n",
    "df_nig_panel, nig_em_data = prepare_nig_inputs(ret_daily, bs, df_rf)\n",
    "\n",
    "print(\"Merton panel shape:\", df_merton.shape)\n",
    "print(\"NIG panel shape:\", df_nig_panel.shape)\n",
    "print(\"-\"*40)\n",
    "print(df_merton.head())\n",
    "print(\"-\"*40)\n",
    "print(df_nig_panel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c12f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Accenture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
