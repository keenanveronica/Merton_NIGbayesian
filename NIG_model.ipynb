{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec9d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vkeenan\\OneDrive - Delft University of Technology\\Documents\\University\\QRM\\Accenture Project\\code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from data_import import (\n",
    "    load_data, load_ecb_1y_yield,\n",
    "    fill_liabilities, drop_high_leverage_firms,\n",
    "    prepare_nig_inputs\n",
    ")\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import brentq\n",
    "from scipy.integrate import quad\n",
    "\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3987194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_data] Firms (ret_daily): 46\n",
      "[load_data] Date range (ret_daily): 2012-01-03 .. 2025-12-19\n",
      "[load_data] Coverage min/median/max: 0.999 / 1.000 / 1.000\n",
      "[load_data] liabilities_scale_used: 1e+06\n",
      "[load_data] QA mcap_reported<=0 rows (raw windowed mkt): 62\n",
      "Data has been written to ecb_yc_1y_aaa.xml\n",
      "[drop_high_leverage_firms] agg=median, threshold=8.0\n",
      "[drop_high_leverage_firms] firms before: 46 | after: 36\n",
      "[drop_high_leverage_firms] dropped firms: 10\n",
      "    gvkey       date             E          isin  \\\n",
      "0  100022 2012-01-03  3.328431e+10  DE0005190003   \n",
      "1  100080 2012-01-03  4.268705e+10  DE000BAY0017   \n",
      "2  100312 2012-01-03  1.469717e+09  DE0007030009   \n",
      "3  100581 2012-01-03  4.935351e+10  FR0000120321   \n",
      "4  100957 2012-01-03  2.931851e+10  ES0144580Y14   \n",
      "\n",
      "                        company country_iso         r             L  \n",
      "0  BAYERISCHE MOTOREN WERKE AKT         DEU  0.001177  8.576700e+10  \n",
      "1                      BAYER AG         DEU  0.001177  3.254300e+10  \n",
      "2                RHEINMETALL AG         DEU  0.001177  3.105000e+09  \n",
      "3                     LOREAL SA         FRA  0.001177  9.178700e+09  \n",
      "4                  IBERDROLA SA         ESP  0.001177  6.203788e+10  \n",
      "(131155, 8)\n",
      "                                date             E              r  \\\n",
      "count                         131155  1.311550e+05  131155.000000   \n",
      "mean   2018-12-26 16:17:25.016964608  7.470086e+10       0.002988   \n",
      "min              2012-01-03 00:00:00  1.215293e+09      -0.009130   \n",
      "25%              2015-06-30 00:00:00  3.097716e+10      -0.006733   \n",
      "50%              2018-12-27 00:00:00  5.218637e+10      -0.002542   \n",
      "75%              2022-06-23 12:00:00  8.899452e+10       0.002193   \n",
      "max              2025-12-19 00:00:00  5.901056e+11       0.036119   \n",
      "std                              NaN  7.517578e+10       0.013478   \n",
      "\n",
      "                  L  \n",
      "count  1.311550e+05  \n",
      "mean   7.068479e+10  \n",
      "min    7.555000e+08  \n",
      "25%    1.412770e+10  \n",
      "50%    3.579000e+10  \n",
      "75%    8.974100e+10  \n",
      "max    6.900840e+11  \n",
      "std    1.019354e+11  \n"
     ]
    }
   ],
   "source": [
    "# data import and input panel preparation\n",
    "ret_daily, bs, coverage = load_data(\n",
    "    Path.cwd() / \"data/raw/Jan2025_Accenture_Dataset_ErasmusCase.xlsx\",\n",
    "    start_date=\"2012-01-01\",\n",
    "    end_date=\"2025-12-19\",\n",
    "    enforce_coverage=True,\n",
    "    coverage_tol=0.95,\n",
    "    liabilities_scale=\"auto\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "df_rf = load_ecb_1y_yield(\n",
    "    startPeriod=\"2010-01-01\",\n",
    "    endPeriod=\"2025-12-31\",\n",
    "    out_file=\"ecb_yc_1y_aaa.xml\",\n",
    "    verify_ssl=True,  # recommended if it works\n",
    ")\n",
    "\n",
    "df_cal = ret_daily[[\"date\"]].drop_duplicates().sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "debt_daily = fill_liabilities(bs, df_cal)\n",
    "\n",
    "ret_filt, bs_filt, lev_by_firm, dropped = drop_high_leverage_firms(\n",
    "    ret_daily,\n",
    "    bs,\n",
    "    df_calendar=df_cal,\n",
    "    debt_daily=debt_daily,\n",
    "    lev_threshold=8.0,\n",
    "    lev_agg=\"median\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# keep debt panel consistent with filtered firms\n",
    "keep = set(ret_filt[\"gvkey\"].astype(str).unique())\n",
    "debt_daily_filt = debt_daily[debt_daily[\"gvkey\"].astype(str).isin(keep)].copy()\n",
    "\n",
    "nig_df, em_cache = prepare_nig_inputs(ret_filt, bs_filt, df_rf, debt_daily=debt_daily_filt, build_em=False)\n",
    "print(nig_df.head())\n",
    "print(nig_df.shape)\n",
    "print(nig_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075268ed",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "For each firm/date, solve for \\(A_t\\) in:\n",
    "\n",
    "$$\n",
    "E_t = C_{\\text{NIG}}(A_t, L_t, r_t, \\tau; \\vartheta_A),\n",
    "\\qquad\n",
    "\\tau = T - t\n",
    "$$\n",
    "\n",
    "where \\(C_{\\text{NIG}}\\) is the NIG‑based equity‑as‑call valuation used by Ahčan & Jovan (Eq. 26).\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Daily asset values**\n",
    "\n",
    "$$\n",
    "\\hat{A}_t\n",
    "$$\n",
    "\n",
    "**Asset log returns**\n",
    "\n",
    "$$\n",
    "\\Delta \\ln \\hat{A}_t\n",
    "$$\n",
    "\n",
    "#### NIG Components\n",
    "\n",
    "**NIG log‑mgf**\n",
    "\n",
    "$$\n",
    "\\kappa(u)\n",
    "$$\n",
    "\n",
    "**Esscher parameter \\(\\theta\\)**  \n",
    "Solving the martingale condition (risk‑neutralization)\n",
    "\n",
    "$$\n",
    "\\theta \\text{ such that the RN drift condition holds}\n",
    "$$\n",
    "\n",
    "**NIG call price**\n",
    "\n",
    "$$\n",
    "C_{\\text{NIG}}(A, L, r, \\tau)\n",
    "$$\n",
    "\n",
    "via a robust Fourier probability representation.\n",
    "\n",
    "**Asset inversion**\n",
    "\n",
    "$$\n",
    "E \\;\\longmapsto\\; \\hat{A}\n",
    "$$\n",
    "\n",
    "via Brent root finding (same spirit as the paper’s “uniroot” step).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c85139",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class NIGParams:\n",
    "    # NIG for log-returns Z: (alpha, beta, delta, mu) under P, per unit time\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    delta: float\n",
    "    mu: float\n",
    "\n",
    "    def validate(self):\n",
    "        if not (self.alpha > abs(self.beta)):\n",
    "            raise ValueError(\"Need alpha > |beta| for NIG.\")\n",
    "        if not (self.delta > 0):\n",
    "            raise ValueError(\"Need delta > 0 for NIG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7bed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nig_kappa(u, p: NIGParams, tau: float):\n",
    "    \"\"\"\n",
    "    Vectorized log-mgf kappa(u; tau) for NIG increments.\n",
    "    u can be scalar or numpy array (real/complex).\n",
    "    \"\"\"\n",
    "    p.validate()\n",
    "    a, b, d, m = p.alpha, p.beta, p.delta, p.mu\n",
    "\n",
    "    u = np.asarray(u, dtype=np.complex128)\n",
    "\n",
    "    term0 = np.sqrt(a*a - b*b)\n",
    "    term1 = np.sqrt(a*a - (b + u)*(b + u))\n",
    "    return (m * u + d * (term0 - term1)) * tau\n",
    "\n",
    "\n",
    "def solve_esscher_theta(p: NIGParams, r: float, tau: float, *, bracket=(-10.0, 10.0)) -> float:\n",
    "    \"\"\"\n",
    "    Solve for theta in:\n",
    "      kappa(theta+1) - kappa(theta) = r*tau\n",
    "    \"\"\"\n",
    "    lo, hi = bracket\n",
    "\n",
    "    def g(th):\n",
    "        # th is real\n",
    "        return (nig_kappa(th + 1.0, p, tau) - nig_kappa(th, p, tau)).real - (r * tau)\n",
    "\n",
    "    # ensure bracket contains root; expand if needed\n",
    "    f_lo, f_hi = g(lo), g(hi)\n",
    "    tries = 0\n",
    "    while f_lo * f_hi > 0 and tries < 10:\n",
    "        lo *= 2\n",
    "        hi *= 2\n",
    "        f_lo, f_hi = g(lo), g(hi)\n",
    "        tries += 1\n",
    "\n",
    "    if f_lo * f_hi > 0:\n",
    "        raise RuntimeError(\"Could not bracket theta root; try different initial NIG params or bracket.\")\n",
    "\n",
    "    return float(brentq(g, lo, hi, maxiter=200))\n",
    "\n",
    "\n",
    "def cf_logA_T_vec(u, lnA: float, p: NIGParams, theta: float, tau: float):\n",
    "    \"\"\"\n",
    "    Vectorized characteristic function of ln A_T under Esscher tilt theta.\n",
    "    u can be scalar or numpy array (real/complex).\n",
    "    \"\"\"\n",
    "    u = np.asarray(u, dtype=np.complex128)\n",
    "    iu = 1j * u\n",
    "    psi = nig_kappa(theta + iu, p, tau) - nig_kappa(theta, p, tau)\n",
    "    return np.exp(1j * u * lnA + psi)\n",
    "\n",
    "\n",
    "def _Pj_prob(j, A, K, p, theta, tau, *, U=120.0, n=8000) -> float:\n",
    "    if j not in (1, 2):\n",
    "        raise ValueError(\"j must be 1 or 2\")\n",
    "    if A <= 0 or K <= 0 or tau <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    lnA = float(np.log(A))\n",
    "    lnK = float(np.log(K))\n",
    "\n",
    "    u = np.linspace(1e-8, U, n)\n",
    "\n",
    "    shift = -1j * (j - 1)\n",
    "    phi = cf_logA_T_vec(u + shift, lnA, p, theta, tau)\n",
    "    phi_shift = cf_logA_T_vec(shift, lnA, p, theta, tau)\n",
    "\n",
    "    expo = np.exp(-1j * u * lnK)\n",
    "    q = expo * phi / (1j * u * phi_shift)\n",
    "\n",
    "    integral = np.trapezoid(np.real(q), u)\n",
    "    Pj = 0.5 + (1.0 / np.pi) * integral\n",
    "    return float(np.clip(Pj, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def call_nig_with_theta(A, K, r, tau, p, theta, *, U=120.0, n=8000) -> float:\n",
    "    P1 = _Pj_prob(1, A, K, p, theta, tau, U=U, n=n)\n",
    "    P2 = _Pj_prob(2, A, K, p, theta, tau, U=U, n=n)\n",
    "\n",
    "    C = A * P1 - K * np.exp(-r * tau) * P2\n",
    "\n",
    "    lower = max(A - K * np.exp(-r * tau), 0.0)\n",
    "    upper = A\n",
    "\n",
    "    if not np.isfinite(C):\n",
    "        return np.nan\n",
    "    return float(min(upper, max(lower, C)))\n",
    "\n",
    "\n",
    "def invert_asset_one_date(E_obs, L, r, tau, p, *, A_prev=None, U=120.0, n=8000, bracket_mult=5.0):\n",
    "    theta = solve_esscher_theta(p, r, tau)\n",
    "\n",
    "    A0 = max(E_obs + 1e-12, E_obs + L, (A_prev if A_prev is not None else 0.0))\n",
    "    A_lo = max(1e-12, 0.1 * L)\n",
    "    A_hi = max(A0, E_obs + bracket_mult * L)\n",
    "\n",
    "    def f(A):\n",
    "        return call_nig_with_theta(A, L, r, tau, p, theta, U=U, n=n) - E_obs\n",
    "\n",
    "    f_lo, f_hi = f(A_lo), f(A_hi)\n",
    "    tries = 0\n",
    "    while f_lo * f_hi > 0 and tries < 20:\n",
    "        A_hi *= 2.0\n",
    "        f_hi = f(A_hi)\n",
    "        tries += 1\n",
    "\n",
    "    if f_lo * f_hi > 0:\n",
    "        raise RuntimeError(\"Could not bracket root for A.\")\n",
    "\n",
    "    A_hat = float(brentq(f, A_lo, A_hi, maxiter=200))\n",
    "    return A_hat, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a20c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2012-01-31\n",
      "E_obs: 39364465866.44\n",
      "L: 85767000000.0\n",
      "r: 0.0020614851688972656\n",
      "A_hat: 124954840585.7082\n",
      "39364465866.44\n"
     ]
    }
   ],
   "source": [
    "# Pick BMW\n",
    "gv = \"100022\"\n",
    "g = nig_df[nig_df[\"gvkey\"].astype(str) == gv].sort_values(\"date\").copy()\n",
    "\n",
    "# pick one month-end date (last date of a month available in your trading calendar)\n",
    "g[\"month\"] = g[\"date\"].dt.to_period(\"M\")\n",
    "month_ends = g.groupby(\"month\")[\"date\"].max().sort_values()\n",
    "asof = month_ends.iloc[0]  # earliest month-end in sample (change as you like)\n",
    "\n",
    "row = g[g[\"date\"] == asof].iloc[0]\n",
    "E_obs, L, r = float(row[\"E\"]), float(row[\"L\"]), float(row[\"r\"])\n",
    "tau = 1.0  # 1Y horizon in years (you can later use ACT/365 convention, but 1.0 is fine for Milestone 1)\n",
    "\n",
    "# Starter NIG parameters (placeholder, just for Milestone 1 stability)\n",
    "# Must satisfy alpha > |beta| and delta > 0.\n",
    "p0 = NIGParams(alpha=15.0, beta=-3.0, delta=0.20, mu=0.00)\n",
    "\n",
    "A_hat, theta = invert_asset_one_date(E_obs, L, r, tau, p0, U=150.0)\n",
    "print(\"Date:\", asof.date())\n",
    "print(\"E_obs:\", E_obs)\n",
    "print(\"L:\", L)\n",
    "print(\"r:\", r)\n",
    "print(\"A_hat:\", A_hat)\n",
    "print(call_nig_with_theta(A_hat, L, r, tau, p0, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215ff11",
   "metadata": {},
   "source": [
    "We now have, for a single date:\n",
    "\n",
    "$$\n",
    "E_t \\;\\longrightarrow\\; \\hat{A}_t\n",
    "$$\n",
    "\n",
    "We will now extend this to a full time series:\n",
    "\n",
    "$$\n",
    "\\{E_t\\}_{t \\in \\text{month-ends}}\n",
    "\\;\\longrightarrow\\;\n",
    "\\{\\hat{A}_t\\}_{t \\in \\text{month-ends}}\n",
    "\\;\\longrightarrow\\;\n",
    "\\Delta \\ln \\hat{A}_t\n",
    "$$\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "- **Monthly asset levels**\n",
    "\n",
    "$$\n",
    "\\{\\hat{A}_t\\}_{t \\in \\text{month-ends}}\n",
    "$$\n",
    "\n",
    "- **Monthly asset log‑returns**\n",
    "\n",
    "$$\n",
    "\\Delta \\ln \\hat{A}_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e4c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_assets_monthly_for_firm(\n",
    "    g: pd.DataFrame,\n",
    "    p: NIGParams,\n",
    "    tau: float = 1.0,\n",
    "    U: float = 150.0,\n",
    "):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    g[\"month\"] = g[\"date\"].dt.to_period(\"M\")\n",
    "    month_ends = g.groupby(\"month\")[\"date\"].max().sort_values()\n",
    "\n",
    "    results = []\n",
    "    A_prev = None\n",
    "\n",
    "    for d in month_ends:\n",
    "        row = g[g[\"date\"] == d].iloc[0]\n",
    "        E_obs = float(row[\"E\"])\n",
    "        L     = float(row[\"L\"])\n",
    "        r     = float(row[\"r\"])\n",
    "\n",
    "        # unpack two outputs\n",
    "        A_hat, theta = invert_asset_one_date(\n",
    "            E_obs, L, r, tau, p,\n",
    "            A_prev=A_prev,\n",
    "            U=U\n",
    "        )\n",
    "\n",
    "        results.append((d, E_obs, L, r, A_hat, theta))\n",
    "        A_prev = A_hat\n",
    "\n",
    "    out = pd.DataFrame(results, columns=[\"date\", \"E\", \"L\", \"r\", \"A_hat\", \"theta\"])\n",
    "    out[\"logA\"] = np.log(out[\"A_hat\"])\n",
    "    out[\"logret_A\"] = out[\"logA\"].diff()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d5ca16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'invert_assets_monthly_for_firm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      7\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m bmw_assets = \u001b[43minvert_assets_monthly_for_firm\u001b[49m(g, p0, U=\u001b[32m120.0\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRuntime:\u001b[39m\u001b[33m\"\u001b[39m, time.time() - start)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(bmw_assets.head())\n",
      "\u001b[31mNameError\u001b[39m: name 'invert_assets_monthly_for_firm' is not defined"
     ]
    }
   ],
   "source": [
    "gv = \"100022\"\n",
    "g = nig_df[nig_df[\"gvkey\"].astype(str) == gv].copy()\n",
    "\n",
    "p0 = NIGParams(alpha=15.0, beta=-3.0, delta=0.20, mu=0.00)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "bmw_assets = invert_assets_monthly_for_firm(g, p0, U=120.0)\n",
    "\n",
    "print(\"Runtime:\", time.time() - start)\n",
    "print(bmw_assets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fdd6e",
   "metadata": {},
   "source": [
    "Keep monthly as baseline, but add extra inversion dates during stress  \n",
    "(without going daily).  \n",
    "This updates the mapping\n",
    "\n",
    "$$\n",
    "E \\;\\longmapsto\\; A\n",
    "$$\n",
    "\n",
    "more frequently when equity dynamics suggest the latent asset process may have shifted.\n",
    "\n",
    "##### Trigger Design\n",
    "\n",
    "Use equity log‑returns (already in your panel as `logret_mcap`) and define triggers when:\n",
    "\n",
    "#### **Large absolute daily move**\n",
    "\n",
    "$$\n",
    "|\\Delta \\log E_t| \\;>\\; q_{0.99}\\big(|\\Delta \\log E|\\big)\n",
    "$$\n",
    "\n",
    "where the quantile is computed **rolling over the last 252 trading days**, firm‑specific.\n",
    "\n",
    "#### Inversion Calendar\n",
    "\n",
    "- all **month‑end** trading dates  \n",
    "- **plus all trigger dates**  \n",
    "- *(optional)* plus the next **1–3 days** after a trigger to capture aftershocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6f34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inversion_calendar(\n",
    "    g_panel: pd.DataFrame,          # nig_df slice for firm (has date, E, L, r)\n",
    "    g_ret: pd.DataFrame,            # ret_filt slice for firm (has date, logret_mcap)\n",
    "    *,\n",
    "    ret_col: str = \"logret_mcap\",\n",
    "    roll_window: int = 252,\n",
    "    q: float = 0.99,\n",
    "    add_post_days: int = 2,\n",
    "):\n",
    "    g_panel = g_panel.sort_values(\"date\").copy()\n",
    "    g_ret = g_ret.sort_values(\"date\").copy()\n",
    "\n",
    "    # month-end trading dates from the panel calendar\n",
    "    month_end = g_panel.groupby(g_panel[\"date\"].dt.to_period(\"M\"))[\"date\"].max()\n",
    "\n",
    "    # rolling quantile threshold for |returns|\n",
    "    absr = g_ret[ret_col].abs()\n",
    "    thr = absr.rolling(roll_window, min_periods=roll_window // 2).quantile(q)\n",
    "\n",
    "    trigger_dates = g_ret.loc[absr > thr, \"date\"]\n",
    "\n",
    "    # add post-trigger days based on the panel’s trading-date index (safer)\n",
    "    if add_post_days > 0:\n",
    "        dates = g_panel[\"date\"].to_numpy()\n",
    "        date_to_pos = {d: i for i, d in enumerate(dates)}\n",
    "        extra = []\n",
    "        for d in trigger_dates:\n",
    "            i = date_to_pos.get(d, None)\n",
    "            if i is None:\n",
    "                continue\n",
    "            for k in range(1, add_post_days + 1):\n",
    "                if i + k < len(dates):\n",
    "                    extra.append(dates[i + k])\n",
    "        trigger_dates = pd.to_datetime(pd.Index(trigger_dates.tolist() + extra)).drop_duplicates()\n",
    "\n",
    "    cal = (\n",
    "        pd.to_datetime(pd.Index(month_end.tolist()).append(pd.Index(trigger_dates)))\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "    )\n",
    "    return pd.DatetimeIndex(cal)\n",
    "\n",
    "\n",
    "def invert_assets_on_calendar_for_firm(\n",
    "    g: pd.DataFrame,\n",
    "    p: NIGParams,\n",
    "    dates: pd.DatetimeIndex,\n",
    "    *,\n",
    "    tau: float = 1.0,\n",
    "    U: float = 120.0,\n",
    "    n: int = 8000,\n",
    "):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    g = g.set_index(\"date\")\n",
    "\n",
    "    results = []\n",
    "    A_prev = None\n",
    "\n",
    "    for d in dates:\n",
    "        if d not in g.index:\n",
    "            continue\n",
    "        row = g.loc[d]\n",
    "        E_obs = float(row[\"E\"])\n",
    "        L     = float(row[\"L\"])\n",
    "        r     = float(row[\"r\"])\n",
    "\n",
    "        A_hat, theta = invert_asset_one_date(\n",
    "            E_obs, L, r, tau, p,\n",
    "            A_prev=A_prev,\n",
    "            U=U, n=n\n",
    "        )\n",
    "        results.append((d, E_obs, L, r, A_hat, theta))\n",
    "        A_prev = A_hat\n",
    "\n",
    "    out = pd.DataFrame(results, columns=[\"date\", \"E\", \"L\", \"r\", \"A_hat\", \"theta\"]).sort_values(\"date\")\n",
    "    out[\"logA\"] = np.log(out[\"A_hat\"])\n",
    "    out[\"logret_A\"] = out[\"logA\"].diff()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31645b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date             E             L         r         A_hat     theta  \\\n",
      "0 2012-01-31  3.936447e+10  8.576700e+10  0.002061  1.249548e+11  2.654517   \n",
      "1 2012-02-29  4.179653e+10  8.576700e+10  0.001470  1.274375e+11  2.610187   \n",
      "2 2012-03-30  4.059254e+10  9.632600e+10  0.001602  1.367644e+11  2.620070   \n",
      "3 2012-04-30  4.322928e+10  9.632600e+10  0.001019  1.394572e+11  2.576356   \n",
      "4 2012-05-31  3.678191e+10  9.632600e+10  0.000512  1.330586e+11  2.538390   \n",
      "\n",
      "        logA  logret_A  \n",
      "0  25.551218       NaN  \n",
      "1  25.570892  0.019674  \n",
      "2  25.641525  0.070633  \n",
      "3  25.661024  0.019498  \n",
      "4  25.614055 -0.046968  \n",
      "                      date             E             L           r  \\\n",
      "count                  300  3.000000e+02  3.000000e+02  300.000000   \n",
      "mean   2019-01-03 06:00:00  4.857056e+10  1.366155e+11    0.002538   \n",
      "min    2012-01-31 00:00:00  2.355908e+10  8.576700e+10   -0.009130   \n",
      "25%    2015-05-11 18:00:00  4.319166e+10  1.173660e+11   -0.006637   \n",
      "50%    2018-09-27 12:00:00  4.834120e+10  1.411720e+11   -0.002635   \n",
      "75%    2022-03-16 00:00:00  5.343159e+10  1.551380e+11    0.001478   \n",
      "max    2025-12-19 00:00:00  7.220932e+10  1.727290e+11    0.035146   \n",
      "std                    NaN  9.058136e+09  2.407006e+10    0.013414   \n",
      "\n",
      "              A_hat       theta        logA    logret_A  \n",
      "count  3.000000e+02  300.000000  300.000000  299.000000  \n",
      "mean   1.847439e+11    2.686011   25.933863    0.001919  \n",
      "min    1.249548e+11    1.816356   25.551218   -0.046968  \n",
      "25%    1.675069e+11    2.002773   25.844290   -0.008253  \n",
      "50%    1.894188e+11    2.302512   25.967226    0.000000  \n",
      "75%    2.026223e+11    2.610768   26.034609    0.009304  \n",
      "max    2.218076e+11    5.094710   26.125076    0.095827  \n",
      "std    2.312749e+10    0.996342    0.132027    0.018312  \n",
      "0.500    0.008638\n",
      "0.900    0.027239\n",
      "0.990    0.056159\n",
      "0.999    0.115964\n",
      "Name: logret_mcap, dtype: float64\n",
      "0.500    0.010575\n",
      "0.900    0.048832\n",
      "0.990    0.118332\n",
      "0.999    0.137806\n",
      "Name: logret_mcap, dtype: float64\n",
      "share of trigger days: 0.08232711306256861\n"
     ]
    }
   ],
   "source": [
    "gv = \"100022\"\n",
    "\n",
    "g_panel = nig_df[nig_df[\"gvkey\"].astype(str) == gv].copy()\n",
    "g_ret   = ret_filt[ret_filt[\"gvkey\"].astype(str) == gv].copy()\n",
    "g_ret = g_ret[[\"date\", \"logret_mcap\"]].copy()\n",
    "g_ret = g_panel[[\"date\"]].merge(g_ret, on=\"date\", how=\"left\")\n",
    "\n",
    "dates = build_inversion_calendar(g_panel, g_ret, q=0.99, add_post_days=2)\n",
    "bmw_assets_trig = invert_assets_on_calendar_for_firm(g_panel, p0, dates)\n",
    "\n",
    "print(bmw_assets_trig.head())\n",
    "print(bmw_assets_trig.describe())\n",
    "\n",
    "abs_all = g_ret[\"logret_mcap\"].abs()\n",
    "abs_trig = g_ret[g_ret[\"date\"].isin(dates)][\"logret_mcap\"].abs()\n",
    "print(abs_all.quantile([0.5, 0.9, 0.99, 0.999]))\n",
    "print(abs_trig.quantile([0.5, 0.9, 0.99, 0.999]))\n",
    "print(\"share of trigger days:\", len(abs_trig)/len(abs_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a24ed5",
   "metadata": {},
   "source": [
    "To improve computational efficiency while preserving statistical validity, we use an event-driven inversion calendar to update the implied asset level A^t more frequently during periods of large equity moves. However, because trigger dates oversample the tails of the equity return distribution (selection bias), we do not use the trigger-subsampled series to estimate the NIG parameters. Instead, NIG parameters are estimated on a fixed, regular return grid (e.g., weekly or daily rolling windows), and the trigger mechanism is used only as a state-update device for the equity-to-asset inversion step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd224cf",
   "metadata": {},
   "source": [
    "#### EM Input Requirements\n",
    "\n",
    "What the EM‑input extractor expects:\n",
    "\n",
    "**Equity series**\n",
    "\n",
    "$$\n",
    "\\{E_t\\}_{t = 1}^{505}\n",
    "$$\n",
    "\n",
    "(last 505 observations)\n",
    "\n",
    "**Risk‑free rate series**\n",
    "\n",
    "$$\n",
    "\\{r_t\\}_{t = 1}^{505}\n",
    "$$\n",
    "\n",
    "(last 505 observations)\n",
    "\n",
    "**Liability level (scalar)**\n",
    "\n",
    "$$\n",
    "L\n",
    "$$\n",
    "\n",
    "chosen over the window (i.e. last value).\n",
    "\n",
    "#### Weekly EM Workflow\n",
    "\n",
    "Define EM dates — for example:\n",
    "\n",
    "- every **Friday**, or  \n",
    "- every **5th trading day**.\n",
    "\n",
    "For each EM date:\n",
    "\n",
    "1. Call  \n",
    "   ```\n",
    "   make_em_inputs(nig_df, gvkey, end_date=em_date, window=505, ...)\n",
    "   ```\n",
    "2. Run your EM initializer / estimator (the function referenced in your docstring).\n",
    "3. Store fitted parameters in a cache:\n",
    "   ```\n",
    "   { em_date : p_hat }\n",
    "   ```\n",
    "\n",
    "##### Using EM Parameters During Asset Inversion\n",
    "\n",
    "When inverting assets on the inversion calendar, use the **most recent** parameter estimate:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{EM date}} \\quad \\text{such that} \\quad \\text{EM date} \\le \\text{inversion date}\n",
    "$$\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **EM schedule:** weekly, fixed  \n",
    "- **Inversion schedule:** month‑ends + triggers  \n",
    "- Both reuse your existing working components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21b3e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729 weekly points\n",
      "                      date             E             L           r  \\\n",
      "count                  729  7.290000e+02  7.290000e+02  729.000000   \n",
      "mean   2018-12-28 00:00:00  4.833134e+10  1.373487e+11    0.002991   \n",
      "min    2012-01-06 00:00:00  2.490454e+10  8.576700e+10   -0.009079   \n",
      "25%    2015-07-03 00:00:00  4.347007e+10  1.173660e+11   -0.006755   \n",
      "50%    2018-12-28 00:00:00  4.821982e+10  1.411720e+11   -0.002541   \n",
      "75%    2022-06-24 00:00:00  5.307190e+10  1.556380e+11    0.002141   \n",
      "max    2025-12-19 00:00:00  7.220932e+10  1.727290e+11    0.035827   \n",
      "std                    NaN  7.755054e+09  2.489014e+10    0.013500   \n",
      "\n",
      "              A_hat       theta        logA    logret_A       dlogA  \n",
      "count  7.290000e+02  729.000000  729.000000  728.000000  728.000000  \n",
      "mean   1.851776e+11    2.719850   25.934448    0.000855    0.000855  \n",
      "min    1.190473e+11    1.820191   25.502787   -0.049308   -0.049308  \n",
      "25%    1.699347e+11    1.993925   25.858680   -0.005845   -0.005845  \n",
      "50%    1.929438e+11    2.309524   25.985665    0.000317    0.000317  \n",
      "75%    2.050515e+11    2.660493   26.046527    0.006518    0.006518  \n",
      "max    2.234569e+11    5.143422   26.132485    0.106333    0.106333  \n",
      "std    2.518579e+10    1.002879    0.145915    0.012395    0.012395  \n"
     ]
    }
   ],
   "source": [
    "def build_weekly_calendar(g_panel: pd.DataFrame) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Weekly inversion/EM grid = last trading day of each week\n",
    "    (robust to holidays because it uses observed trading dates).\n",
    "    \"\"\"\n",
    "    d = g_panel[[\"date\"]].drop_duplicates().sort_values(\"date\")\n",
    "    week_end = d.groupby(d[\"date\"].dt.to_period(\"W\"))[\"date\"].max()\n",
    "    return pd.DatetimeIndex(week_end.sort_values())\n",
    "\n",
    "\n",
    "def build_weekly_em_windows(\n",
    "    assets_weekly: pd.DataFrame,\n",
    "    *,\n",
    "    window_weeks: int = 104,   # ~2 years weekly\n",
    "    L_pick: str = \"last\",      # or \"median\"\n",
    "):\n",
    "    df = assets_weekly.sort_values(\"date\").copy()\n",
    "    df = df.dropna(subset=[\"dlogA\", \"r\", \"L\"])\n",
    "\n",
    "    em_inputs = {}\n",
    "    for i in range(window_weeks, len(df) + 1):\n",
    "        w = df.iloc[i-window_weeks:i]\n",
    "        end_date = w[\"date\"].iloc[-1]\n",
    "\n",
    "        dlogA = w[\"dlogA\"].to_numpy(float)\n",
    "        r_arr = w[\"r\"].to_numpy(float)\n",
    "        if L_pick == \"last\":\n",
    "            L_scalar = float(w[\"L\"].iloc[-1])\n",
    "        elif L_pick == \"median\":\n",
    "            L_scalar = float(w[\"L\"].median())\n",
    "        else:\n",
    "            raise ValueError(\"L_pick must be 'last' or 'median'\")\n",
    "\n",
    "        em_inputs[pd.to_datetime(end_date)] = (dlogA, L_scalar, r_arr)\n",
    "\n",
    "    return em_inputs\n",
    "\n",
    "\n",
    "gv = \"100022\"\n",
    "g_panel = nig_df[nig_df[\"gvkey\"].astype(str) == gv].copy()\n",
    "\n",
    "p0 = NIGParams(alpha=15.0, beta=-3.0, delta=0.20, mu=0.00)\n",
    "\n",
    "dates_weekly = build_weekly_calendar(g_panel)\n",
    "bmw_assets_weekly = invert_assets_on_calendar_for_firm(g_panel, p0, dates_weekly)\n",
    "\n",
    "bmw_assets_weekly[\"logA\"] = np.log(bmw_assets_weekly[\"A_hat\"])\n",
    "bmw_assets_weekly[\"dlogA\"] = bmw_assets_weekly[\"logA\"].diff()\n",
    "\n",
    "print(len(dates_weekly), \"weekly points\")\n",
    "print(bmw_assets_weekly.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1eece24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM windows: 625\n"
     ]
    }
   ],
   "source": [
    "assert {\"r\",\"L\",\"A_hat\",\"dlogA\",\"date\"}.issubset(bmw_assets_weekly.columns)\n",
    "em_inputs = build_weekly_em_windows(bmw_assets_weekly, window_weeks=104)\n",
    "print(\"EM windows:\", len(em_inputs))  # should be about 729-104 ≈ 625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26d6be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import brentq\n",
    "from scipy.special import kve  # exponentially-scaled modified Bessel K\n",
    "\n",
    "@dataclass\n",
    "class NIGParams:\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    delta: float\n",
    "    mu: float\n",
    "\n",
    "    def validate(self):\n",
    "        if not (self.alpha > abs(self.beta) and self.delta > 0):\n",
    "            raise ValueError(\"Need alpha > |beta| and delta > 0.\")\n",
    "\n",
    "def _safe_ratio_kve(nu_num, nu_den, x):\n",
    "    # ratio K_nu_num(x) / K_nu_den(x) using scaled kve; scaling cancels in ratio\n",
    "    return kve(nu_num, x) / kve(nu_den, x)\n",
    "\n",
    "def em_fit_nig(\n",
    "    x: np.ndarray,\n",
    "    p_start: NIGParams,\n",
    "    *,\n",
    "    max_iter: int = 200,\n",
    "    tol: float = 1e-6,\n",
    "    min_alpha_gap: float = 1e-6,\n",
    "    verbose: bool = False,\n",
    ") -> NIGParams:\n",
    "    \"\"\"\n",
    "    EM for NIG on 1D returns array x (e.g., weekly dlogA).\n",
    "    Uses GIG posterior moments with Bessel K ratios.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size < 10:\n",
    "        raise ValueError(\"Too few observations for EM.\")\n",
    "\n",
    "    p = NIGParams(**vars(p_start))\n",
    "    p.validate()\n",
    "\n",
    "    n = x.size\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # current derived parameter\n",
    "        gamma = np.sqrt(max(p.alpha*p.alpha - p.beta*p.beta, 1e-18))\n",
    "        psi = gamma*gamma  # = alpha^2 - beta^2\n",
    "\n",
    "        # E-step: Y|X is GIG(lambda=-1/2, chi=delta^2+(x-mu)^2, psi=gamma^2)\n",
    "        chi = p.delta*p.delta + (x - p.mu)**2\n",
    "        chi = np.maximum(chi, 1e-18)\n",
    "        s = np.sqrt(chi * psi)\n",
    "        s = np.maximum(s, 1e-12)\n",
    "\n",
    "        lam = -0.5\n",
    "        # Moments:\n",
    "        # E[Y]   = (K_{lam+1}(s)/K_lam(s)) * sqrt(chi/psi)\n",
    "        # E[1/Y] = (K_{lam-1}(s)/K_lam(s)) * sqrt(psi/chi)\n",
    "        ratio_p1 = _safe_ratio_kve(lam + 1.0, lam, s)   # K_{+1/2}/K_{-1/2}\n",
    "        ratio_m1 = _safe_ratio_kve(lam - 1.0, lam, s)   # K_{-3/2}/K_{-1/2}\n",
    "\n",
    "        Ey  = ratio_p1 * np.sqrt(chi / psi)\n",
    "        E1y = ratio_m1 * np.sqrt(psi / chi)\n",
    "\n",
    "        # M-step for mu, beta (solve 2x2 linear system derived from normal equations)\n",
    "        W  = np.sum(E1y)\n",
    "        A  = np.sum(E1y * x)\n",
    "        X  = np.sum(x)\n",
    "        Y  = np.sum(Ey)\n",
    "\n",
    "        denom = (n*n / Y) - W\n",
    "        if abs(denom) < 1e-18:\n",
    "            # fallback: tiny ridge\n",
    "            denom = np.sign(denom) * 1e-18 if denom != 0 else 1e-18\n",
    "\n",
    "        mu_new = (n * X / Y - A) / denom\n",
    "        beta_new = (X - n * mu_new) / Y\n",
    "\n",
    "        # M-step for delta, gamma (closed form)\n",
    "        S1 = np.sum(E1y)\n",
    "        # denominator must be positive\n",
    "        den_delta = S1 - (n*n / Y)\n",
    "        if den_delta <= 1e-18:\n",
    "            den_delta = 1e-18\n",
    "\n",
    "        delta_new = np.sqrt(n / den_delta)\n",
    "        gamma_new = (n * delta_new) / Y\n",
    "\n",
    "        # recover alpha from gamma and beta\n",
    "        alpha_new = np.sqrt(beta_new*beta_new + gamma_new*gamma_new)\n",
    "\n",
    "        # enforce alpha > |beta|\n",
    "        if alpha_new <= abs(beta_new) + min_alpha_gap:\n",
    "            alpha_new = abs(beta_new) + min_alpha_gap + 1e-6\n",
    "\n",
    "        # convergence check (relative)\n",
    "        vec_old = np.array([p.alpha, p.beta, p.delta, p.mu])\n",
    "        vec_new = np.array([alpha_new, beta_new, delta_new, mu_new])\n",
    "        rel = np.max(np.abs(vec_new - vec_old) / (np.abs(vec_old) + 1e-8))\n",
    "\n",
    "        p = NIGParams(alpha=float(alpha_new), beta=float(beta_new),\n",
    "                      delta=float(delta_new), mu=float(mu_new))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[EM] iter={it:03d} rel_change={rel:.2e} \"\n",
    "                  f\"alpha={p.alpha:.3f} beta={p.beta:.3f} delta={p.delta:.3f} mu={p.mu:.5f}\")\n",
    "\n",
    "        if rel < tol:\n",
    "            break\n",
    "\n",
    "    p.validate()\n",
    "    return p\n",
    "\n",
    "\n",
    "def run_weekly_inversion_plus_em(\n",
    "    g_panel: pd.DataFrame,\n",
    "    *,\n",
    "    p0: NIGParams,\n",
    "    window_weeks: int = 104,\n",
    "    em_max_iter: int = 80,\n",
    "    em_tol: float = 1e-6,\n",
    "    U: float = 120.0,\n",
    "    n: int = 2000\n",
    "):\n",
    "    \"\"\"\n",
    "    Fully iterative weekly loop:\n",
    "    - Invert A_hat on weekly dates using current params p_t\n",
    "    - After enough history, EM-fit p_{t+1} on last window_weeks of dlogA\n",
    "    Returns:\n",
    "      assets_weekly (df with A_hat, theta, dlogA, p_used fields),\n",
    "      p_cache (dict end_date->NIGParams)\n",
    "    \"\"\"\n",
    "    dates = build_weekly_calendar(g_panel)\n",
    "    g_panel = g_panel.sort_values(\"date\").copy()\n",
    "    g_week = g_panel[g_panel[\"date\"].isin(dates)].copy().sort_values(\"date\")\n",
    "\n",
    "    rows = []\n",
    "    p_cache = {}\n",
    "    p_curr = p0\n",
    "    A_prev = None\n",
    "\n",
    "    # store dlogA history for EM\n",
    "    dlogA_hist = []\n",
    "\n",
    "    for i, (_, row) in enumerate(g_week.iterrows()):\n",
    "        d = pd.to_datetime(row[\"date\"])\n",
    "        E_obs = float(row[\"E\"])\n",
    "        L = float(row[\"L\"])\n",
    "        r = float(row[\"r\"])\n",
    "\n",
    "        # 1) Invert assets for this week using current params\n",
    "        A_hat, theta = invert_asset_one_date(\n",
    "            E_obs, L, r, tau=1.0, p=p_curr,\n",
    "            A_prev=A_prev, U=U, n=n\n",
    "        )\n",
    "\n",
    "        logA = np.log(A_hat)\n",
    "        if i == 0:\n",
    "            dlogA = np.nan\n",
    "        else:\n",
    "            dlogA = logA - rows[-1][\"logA\"]\n",
    "\n",
    "        rows.append({\n",
    "            \"date\": d, \"E\": E_obs, \"L\": L, \"r\": r,\n",
    "            \"A_hat\": A_hat, \"theta\": theta,\n",
    "            \"logA\": logA, \"dlogA\": dlogA,\n",
    "            \"p_alpha\": p_curr.alpha, \"p_beta\": p_curr.beta,\n",
    "            \"p_delta\": p_curr.delta, \"p_mu\": p_curr.mu,\n",
    "        })\n",
    "\n",
    "        A_prev = A_hat\n",
    "\n",
    "        # 2) Update EM params once we have enough dlogA history\n",
    "        if np.isfinite(dlogA):\n",
    "            dlogA_hist.append(dlogA)\n",
    "\n",
    "        if len(dlogA_hist) >= window_weeks:\n",
    "            x = np.array(dlogA_hist[-window_weeks:], float)\n",
    "            p_new = em_fit_nig(\n",
    "                x, p_curr,\n",
    "                max_iter=em_max_iter,\n",
    "                tol=em_tol,\n",
    "                verbose=False,\n",
    "            )\n",
    "            p_curr = p_new\n",
    "            p_cache[d] = p_curr\n",
    "\n",
    "    assets_weekly = pd.DataFrame(rows)\n",
    "    return assets_weekly, p_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc15811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date         A_hat     dlogA  p_alpha  p_beta  p_delta  p_mu\n",
      "0 2012-01-06  1.190473e+11       NaN     15.0    -3.0      0.2   0.0\n",
      "1 2012-01-13  1.208258e+11  0.014829     15.0    -3.0      0.2   0.0\n",
      "2 2012-01-20  1.238685e+11  0.024871     15.0    -3.0      0.2   0.0\n",
      "3 2012-01-27  1.244749e+11  0.004884     15.0    -3.0      0.2   0.0\n",
      "4 2012-02-03  1.277593e+11  0.026044     15.0    -3.0      0.2   0.0\n",
      "weeks: 729 em-updates: 625\n"
     ]
    }
   ],
   "source": [
    "gv = \"100022\"\n",
    "g_panel = nig_df[nig_df[\"gvkey\"].astype(str) == gv].copy()\n",
    "\n",
    "p0 = NIGParams(alpha=15.0, beta=-3.0, delta=0.20, mu=0.00)\n",
    "\n",
    "assets_weekly_iter, p_cache_iter = run_weekly_inversion_plus_em(\n",
    "    g_panel,\n",
    "    p0=p0,\n",
    "    window_weeks=104,\n",
    "    em_max_iter=80,   # warm-start -> usually enough\n",
    "    em_tol=1e-6,\n",
    "    U=120.0,\n",
    "    n=2000\n",
    ")\n",
    "\n",
    "print(assets_weekly_iter[[\"date\",\"A_hat\",\"dlogA\",\"p_alpha\",\"p_beta\",\"p_delta\",\"p_mu\"]].head())\n",
    "print(\"weeks:\", len(assets_weekly_iter), \"em-updates:\", len(p_cache_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ad584ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_alpha</th>\n",
       "      <th>p_beta</th>\n",
       "      <th>p_delta</th>\n",
       "      <th>p_mu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>728.000000</td>\n",
       "      <td>7.280000e+02</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>7.280000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000837</td>\n",
       "      <td>4.120923e-03</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>7.525051e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012903</td>\n",
       "      <td>1.113627e-01</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>1.704180e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.345287</td>\n",
       "      <td>-1.046451e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.731436e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000283</td>\n",
       "      <td>-1.432111e-06</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>-6.684920e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-1.313765e-07</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>9.755716e-08</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>6.208313e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.004722e+00</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>2.270922e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          p_alpha        p_beta     p_delta          p_mu\n",
       "count  728.000000  7.280000e+02  728.000000  7.280000e+02\n",
       "mean    -0.000837  4.120923e-03    0.003519  7.525051e-07\n",
       "std      0.012903  1.113627e-01    0.003330  1.704180e-04\n",
       "min     -0.345287 -1.046451e-03    0.000000 -9.731436e-04\n",
       "25%     -0.000283 -1.432111e-06    0.001426 -6.684920e-05\n",
       "50%     -0.000054 -1.313765e-07    0.001895  0.000000e+00\n",
       "75%     -0.000005  9.755716e-08    0.005603  6.208313e-05\n",
       "max      0.000000  3.004722e+00    0.025178  2.270922e-03"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert np.all(assets_weekly_iter[\"p_alpha\"] > np.abs(assets_weekly_iter[\"p_beta\"]))\n",
    "assert np.all(assets_weekly_iter[\"p_delta\"] > 0)\n",
    "assets_weekly_iter[[\"p_alpha\",\"p_beta\",\"p_delta\",\"p_mu\"]].diff().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56081935",
   "metadata": {},
   "source": [
    "*Important note*:\n",
    "- The period 2012–2013 is used as an initial estimation sample to (i) construct a first asset path and (ii) obtain the first NIG parameter estimates via EM.\n",
    "- From 2014 onward, parameters are updated on a weekly grid using a 2-year rolling window; PDs from this period are therefore “fully model-implied” (conditioned on prior 2 years of information)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e01666",
   "metadata": {},
   "source": [
    "### Key Idea\n",
    "\n",
    "If log‑asset return over horizon \\( \\tau \\) is NIG with cumulant\n",
    "\n",
    "$$\n",
    "\\kappa(u;\\tau)\n",
    "=\n",
    "\\tau\\!\\left(\n",
    "\\mu u\n",
    "+\n",
    "\\delta\\!\\left(\n",
    "\\sqrt{\\alpha^{2}-\\beta^{2}}\n",
    "-\n",
    "\\sqrt{\\alpha^{2}-(\\beta+u)^{2}}\n",
    "\\right)\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\ln A_{t+\\tau}\n",
    "=\n",
    "\\ln A_t + X_\\tau,\n",
    "\\qquad\n",
    "X_\\tau \\sim \\text{NIG}(\\alpha,\\beta,\\delta\\tau,\\mu\\tau),\n",
    "$$\n",
    "\n",
    "and the terminal PD at horizon \\( \\tau \\) is\n",
    "\n",
    "$$\n",
    "\\text{PD}_t(\\tau)\n",
    "=\n",
    "\\Pr(A_{t+\\tau} \\le L_t)\n",
    "=\n",
    "\\Pr\\!\\left(\n",
    "X_\\tau \\le \\ln\\!\\frac{L_t}{A_t}\n",
    "\\right)\n",
    "=\n",
    "F_{\\text{NIG}}\\!\\left(\n",
    "\\ln\\!\\frac{L_t}{A_t};\n",
    "\\alpha,\\beta,\\delta\\tau,\\mu\\tau\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "We will compute this using:\n",
    "\n",
    "```\n",
    "scipy.stats.norminvgauss.cdf\n",
    "```\n",
    "\n",
    "**Assumption (important):**  \n",
    "SciPy’s `norminvgauss(a, b, loc, scale)` matches your  \n",
    "\\((\\alpha,\\beta,\\mu,\\delta)\\) as:\n",
    "\n",
    "- `a = alpha`\n",
    "- `b = beta`\n",
    "- `loc = mu`\n",
    "- `scale = delta`\n",
    "\n",
    "This is the standard mapping used in most implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a79316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norminvgauss\n",
    "\n",
    "def pd_1y_terminal_nig(\n",
    "    A_t: float,\n",
    "    L_t: float,\n",
    "    p: NIGParams,\n",
    "    *,\n",
    "    tau_steps: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Terminal PD over tau_steps (in the SAME time units as p was estimated on).\n",
    "\n",
    "    If p is estimated on WEEKLY returns, use tau_steps=52 for 1 year.\n",
    "    If p is estimated on DAILY returns,  use tau_steps=252 for 1 year.\n",
    "    If p is estimated on MONTHLY returns, use tau_steps=12 for 1 year.\n",
    "    \"\"\"\n",
    "    if (A_t <= 0) or (L_t <= 0):\n",
    "        return np.nan\n",
    "\n",
    "    p.validate()\n",
    "    x = float(np.log(L_t / A_t))  # threshold on log-return\n",
    "\n",
    "    # time scaling\n",
    "    delta = float(p.delta) * tau_steps\n",
    "    mu    = float(p.mu) * tau_steps\n",
    "    alpha = float(p.alpha)\n",
    "    beta  = float(p.beta)\n",
    "\n",
    "    # SciPy mapping\n",
    "    a = alpha * delta\n",
    "    b = beta  * delta\n",
    "    loc = mu\n",
    "    scale = delta\n",
    "\n",
    "    pd_val = norminvgauss.cdf(x, a=a, b=b, loc=loc, scale=scale)\n",
    "    return float(np.clip(pd_val, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def month_end_calendar(g_panel: pd.DataFrame) -> pd.DatetimeIndex:\n",
    "    d = g_panel[[\"date\"]].drop_duplicates().sort_values(\"date\")\n",
    "    me = d.groupby(d[\"date\"].dt.to_period(\"M\"))[\"date\"].max()\n",
    "    return pd.DatetimeIndex(me.sort_values())\n",
    "\n",
    "\n",
    "def pd_monthly_one_firm(\n",
    "    g_panel: pd.DataFrame,\n",
    "    *,\n",
    "    gvkey: str,\n",
    "    p0: NIGParams,\n",
    "    p_cache: dict[pd.Timestamp, NIGParams],\n",
    "    # inversion maturity (your option pricing maturity)\n",
    "    tau_inv: float = 1.0,\n",
    "    # PD horizon in \"steps\" matching EM frequency (52 if weekly params)\n",
    "    tau_steps_pd: float = 52.0,\n",
    "    U: float = 120.0,\n",
    "    n: int = 2000,\n",
    "    eta: float = 0.0,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    g = g_panel.sort_values(\"date\").copy()\n",
    "    dates = month_end_calendar(g)\n",
    "\n",
    "    out = []\n",
    "    A_prev = None\n",
    "\n",
    "    for d in dates:\n",
    "        row = g.loc[g[\"date\"] == d].iloc[0]\n",
    "        E_obs = float(row[\"E\"])\n",
    "        L_t   = float(row[\"L\"])\n",
    "        r_t   = float(row[\"r\"])\n",
    "\n",
    "        p_t = pick_params_for_date(p_cache, pd.to_datetime(d), fallback=p0)\n",
    "\n",
    "        A_hat, theta = invert_asset_one_date(\n",
    "            E_obs, L_t, r_t, tau_inv, p_t,\n",
    "            A_prev=A_prev, U=U, n=n,\n",
    "        )\n",
    "\n",
    "        pd_1y = pd_1y_terminal_nig(A_hat, L_t, p_t, tau_steps=tau_steps_pd)\n",
    "\n",
    "        out.append({\n",
    "            \"gvkey\": str(gvkey),\n",
    "            \"date\": pd.to_datetime(d),\n",
    "            \"E\": E_obs,\n",
    "            \"L\": L_t,\n",
    "            \"r\": r_t,\n",
    "            \"A_hat\": float(A_hat),\n",
    "            \"theta\": float(theta),\n",
    "            \"p_alpha\": float(p_t.alpha),\n",
    "            \"p_beta\": float(p_t.beta),\n",
    "            \"p_delta\": float(p_t.delta),\n",
    "            \"p_mu\": float(p_t.mu),\n",
    "            \"PD_1y\": float(pd_1y),\n",
    "        })\n",
    "\n",
    "        A_prev = A_hat\n",
    "\n",
    "    return pd.DataFrame(out).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def pick_params_for_date(\n",
    "    p_cache: dict[pd.Timestamp, NIGParams],\n",
    "    d: pd.Timestamp,\n",
    "    *,\n",
    "    fallback: NIGParams,\n",
    ") -> NIGParams:\n",
    "    if not p_cache:\n",
    "        return fallback\n",
    "\n",
    "    d = pd.to_datetime(d)\n",
    "\n",
    "    # p_cache keys might be Timestamp or datetime64; normalize\n",
    "    keys = sorted(pd.to_datetime(list(p_cache.keys())))\n",
    "    # pick last key <= d\n",
    "    eligible = [k for k in keys if k <= d]\n",
    "    if not eligible:\n",
    "        return fallback\n",
    "\n",
    "    return p_cache[eligible[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b148fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gvkey       date             E             L         r         A_hat  \\\n",
      "0  100022 2012-01-31  3.936447e+10  8.576700e+10  0.002061  1.249548e+11   \n",
      "1  100022 2012-02-29  4.179653e+10  8.576700e+10  0.001470  1.274375e+11   \n",
      "2  100022 2012-03-30  4.059254e+10  9.632600e+10  0.001602  1.367644e+11   \n",
      "3  100022 2012-04-30  4.322928e+10  9.632600e+10  0.001019  1.394572e+11   \n",
      "4  100022 2012-05-31  3.678191e+10  9.632600e+10  0.000512  1.330586e+11   \n",
      "\n",
      "      theta  p_alpha  p_beta  p_delta  p_mu     PD_1y  \n",
      "0  2.654517     15.0    -3.0      0.2   0.0  0.980226  \n",
      "1  2.610187     15.0    -3.0      0.2   0.0  0.979071  \n",
      "2  2.620070     15.0    -3.0      0.2   0.0  0.981658  \n",
      "3  2.576356     15.0    -3.0      0.2   0.0  0.980584  \n",
      "4  2.538390     15.0    -3.0      0.2   0.0  0.983086  \n",
      "             PD_1y  A_over_L\n",
      "PD_1y     1.000000 -0.899992\n",
      "A_over_L -0.899992  1.000000\n",
      "count    144.000000\n",
      "mean       0.450846\n",
      "std        0.030476\n",
      "min        0.341185\n",
      "25%        0.445132\n",
      "50%        0.462761\n",
      "75%        0.470142\n",
      "max        0.479091\n",
      "Name: PD_1y, dtype: float64\n",
      "count    144.000000\n",
      "mean       1.349477\n",
      "std        0.094798\n",
      "min        1.175672\n",
      "25%        1.280574\n",
      "50%        1.339402\n",
      "75%        1.380462\n",
      "max        1.664679\n",
      "Name: A_over_L, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gv = \"100022\"\n",
    "g_panel = nig_df[nig_df[\"gvkey\"].astype(str) == gv].copy()\n",
    "\n",
    "# p0 is your fallback pre-burn-in\n",
    "p0 = NIGParams(alpha=15.0, beta=-3.0, delta=0.20, mu=0.00)\n",
    "\n",
    "bmw_pd_m = pd_monthly_one_firm(\n",
    "    g_panel=g_panel,\n",
    "    gvkey=\"100022\",\n",
    "    p0=p0,\n",
    "    p_cache=p_cache,\n",
    "    tau_inv=1.0,        # your pricing inversion maturity\n",
    "    tau_steps_pd=52.0,  # 1Y PD using weekly EM params\n",
    "    U=120.0,\n",
    "    n=2000,\n",
    ")\n",
    "\n",
    "# Apply evaluation start (burn-in complete): 2014-01-01\n",
    "bmw_pd_eval = bmw_pd_m[bmw_pd_m[\"date\"] >= \"2014-01-01\"].copy()\n",
    "\n",
    "print(bmw_pd_m.head())\n",
    "bmw_pd_eval[\"A_over_L\"] = bmw_pd_eval[\"A_hat\"] / bmw_pd_eval[\"L\"]\n",
    "print(bmw_pd_eval[[\"PD_1y\",\"A_over_L\"]].corr())\n",
    "print(bmw_pd_eval[\"PD_1y\"].describe())\n",
    "print(bmw_pd_eval[\"A_over_L\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e10b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Accenture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
